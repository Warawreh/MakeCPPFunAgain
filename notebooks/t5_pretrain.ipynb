{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885a372",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "#include <filesystem>\n",
    "#include <fstream>\n",
    "#include <random>\n",
    "#include <chrono>\n",
    "#include <string>\n",
    "#include <vector>\n",
    "#include <unordered_map>\n",
    "#include <sstream>\n",
    "#include <iomanip>\n",
    "#include <algorithm>\n",
    "#include <cstdlib>\n",
    "#include <cstdio>\n",
    "#include <regex>\n",
    "\n",
    "#pragma cling add_include_path(\"/home/warawreh/MakeCPPFunAgain/libtorch/include\")\n",
    "#pragma cling add_include_path(\"/home/warawreh/MakeCPPFunAgain/libtorch/include/torch/csrc/api/include\")\n",
    "#pragma cling add_include_path(\"/home/warawreh/MakeCPPFunAgain/include\")\n",
    "#pragma cling add_library_path(\"/home/warawreh/MakeCPPFunAgain/libtorch/lib\")\n",
    "#pragma cling load(\"/home/warawreh/MakeCPPFunAgain/libtorch/lib/libc10.so\")\n",
    "#pragma cling load(\"/home/warawreh/MakeCPPFunAgain/libtorch/lib/libtorch_cpu.so\")\n",
    "#pragma cling load(\"/home/warawreh/MakeCPPFunAgain/libtorch/lib/libtorch.so\")\n",
    "\n",
    "#include \"mcppfa/huggingface.hpp\"\n",
    "#include \"mcppfa/sentencepiece_lite.hpp\"\n",
    "#include \"mcppfa/hf_dataset.hpp\"\n",
    "#include \"mcppfa/hf_trainer.hpp\"\n",
    "#include \"mcppfa/model_summary.hpp\"\n",
    "#include \"mcppfa/torch_bert.hpp\"   // use the T5 implementation in torch_bert.hpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a838a",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "using namespace std;\n",
    "using namespace mcppfa;\n",
    "\n",
    "// Force stdout/stderr to stream while the cell runs (otherwise output may appear only at the end).\n",
    "std::cout.setf(std::ios::unitbuf);\n",
    "std::cerr.setf(std::ios::unitbuf);\n",
    "setvbuf(stdout, nullptr, _IONBF, 0);\n",
    "setvbuf(stderr, nullptr, _IONBF, 0);\n",
    "\n",
    "// --- Globals shared across cells ---\n",
    "std::string g_hf_token;\n",
    "// Use this repo ONLY as tokenizer assets repo (spiece.model + tokenizer_config.json).\n",
    "std::string g_model_repo = \"XXXXX\";\n",
    "std::string g_dataset_repo = \"XXXXXX\";\n",
    "std::string g_revision = \"main\";\n",
    "\n",
    "std::string g_model_dir;\n",
    "std::string g_spiece_path;\n",
    "std::string g_tokenizer_config_path;\n",
    "spm_lite::SentencePieceLite g_sp;\n",
    "bool g_sp_loaded = false;\n",
    "\n",
    "// Tokenizer-derived ids/sizes\n",
    "int64_t g_base_sp_vocab_size = 0;\n",
    "int64_t g_vocab_size = 0;\n",
    "int64_t g_pad_id = 0;\n",
    "\n",
    "// --- Training params (editable in Cell 3) ---\n",
    "int64_t g_max_len = 32;\n",
    "double  g_lr = 1e-3;\n",
    "int     g_epochs = 10;\n",
    "int64_t g_batch_size = 32;\n",
    "\n",
    "size_t g_n_train = 8;\n",
    "size_t g_n_valid = 4;\n",
    "size_t g_n_test  = 4;\n",
    "\n",
    "torchlm::T5Model g_t5 = nullptr;\n",
    "torch::Device g_device = torch::kCPU;\n",
    "bool g_model_ready = false;\n",
    "\n",
    "hf_dataset::Table g_train_tbl;\n",
    "hf_dataset::Table g_valid_tbl;\n",
    "hf_dataset::Table g_test_tbl;\n",
    "bool g_dataset_loaded = false;\n",
    "\n",
    "try {\n",
    "    // Load HF token from secrets.txt (do NOT print it)\n",
    "    try {\n",
    "        g_hf_token = hf::read_token_file(\"secrets.txt\");\n",
    "    } catch (const std::exception& e) {\n",
    "        cerr << \"Warning: could not read secrets.txt (\" << e.what() << \"); private downloads may fail.\" << endl;\n",
    "        g_hf_token.clear();\n",
    "    }\n",
    "    if (!g_hf_token.empty()) {\n",
    "        setenv(\"HF_TOKEN\", g_hf_token.c_str(), 1);\n",
    "        setenv(\"HUGGINGFACE_HUB_TOKEN\", g_hf_token.c_str(), 1);\n",
    "    }\n",
    "\n",
    "    // --- Load tokenizer assets from repo ---\n",
    "    g_model_dir = std::string(\".hf/\") + g_model_repo.substr(g_model_repo.find_last_of('/') + 1);\n",
    "    std::filesystem::create_directories(g_model_dir);\n",
    "    g_spiece_path = g_model_dir + \"/spiece.model\";\n",
    "    g_tokenizer_config_path = g_model_dir + \"/tokenizer_config.json\";\n",
    "\n",
    "    cout << \"Preparing tokenizer from repo: \" << g_model_repo << endl;\n",
    "    {\n",
    "        auto r = hf::download_file_http(g_model_repo, \"spiece.model\", g_spiece_path, hf::RepoType::model, \"main\", g_hf_token);\n",
    "        if (r.exit_code != 0 && !std::filesystem::exists(g_spiece_path)) {\n",
    "            throw std::runtime_error(\"Failed to download spiece.model (and no local copy present)\");\n",
    "        }\n",
    "        // tokenizer_config.json is needed to get the *effective* vocab size (added tokens / extra ids).\n",
    "        auto r2 = hf::download_file_http(g_model_repo, \"tokenizer_config.json\", g_tokenizer_config_path, hf::RepoType::model, \"main\", g_hf_token);\n",
    "        if (r2.exit_code != 0 && !std::filesystem::exists(g_tokenizer_config_path)) {\n",
    "            cerr << \"Warning: could not download tokenizer_config.json; vocab size may be incomplete.\" << endl;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Base SentencePiece vocab comes only from spiece.model (often small in domain-specific tokenizers).\n",
    "    g_sp.load_from_file(g_spiece_path);\n",
    "    g_sp_loaded = (g_sp.vocab_size() > 0);\n",
    "    g_base_sp_vocab_size = static_cast<int64_t>(g_sp.vocab_size());\n",
    "    cout << \"Loaded spiece.model base_vocab_size=\" << g_base_sp_vocab_size << endl;\n",
    "\n",
    "    // Merge HF added tokens (added_tokens_decoder) so ids and vocab_size match HuggingFace tokenizer.\n",
    "    // This is why you were seeing vocab_size=128: that's just the *SentencePiece* piece count;\n",
    "    // HF appends lots of tokens (extra_id_*, pad, and custom domain tokens) outside spiece.model.\n",
    "    int64_t max_added_id = -1;\n",
    "    int64_t extra_ids = 0;\n",
    "    std::string pad_token_str;\n",
    "    if (std::filesystem::exists(g_tokenizer_config_path)) {\n",
    "        std::ifstream in(g_tokenizer_config_path);\n",
    "        std::string cfg((std::istreambuf_iterator<char>(in)), std::istreambuf_iterator<char>());\n",
    "\n",
    "        {\n",
    "            std::smatch m;\n",
    "            if (std::regex_search(cfg, m, std::regex(\"\\\\\\\"extra_ids\\\\\\\"\\\\s*:\\\\s*(\\\\d+)\"))) {\n",
    "                extra_ids = std::stoll(m[1].str());\n",
    "            }\n",
    "        }\n",
    "        {\n",
    "            std::smatch m;\n",
    "            if (std::regex_search(cfg, m, std::regex(\"\\\\\\\"pad_token\\\\\\\"\\\\s*:\\\\s*\\\\\\\"([^\\\\\\\"]+)\\\\\\\"\"))) {\n",
    "                pad_token_str = m[1].str();\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // Parse entries like: \"123\": { \"content\": \"TOKEN\", ... }\n",
    "        // This is a targeted parser for tokenizer_config.json's added_tokens_decoder object.\n",
    "        std::regex entry_re(\"\\\\\\\"(\\\\d+)\\\\\\\"\\\\s*:\\\\s*\\\\{[^\\\\}]*?\\\\\\\"content\\\\\\\"\\\\s*:\\\\s*\\\\\\\"([^\\\\\\\"]*)\\\\\\\"\");\n",
    "        for (auto it = std::sregex_iterator(cfg.begin(), cfg.end(), entry_re); it != std::sregex_iterator(); ++it) {\n",
    "            const int64_t id = std::stoll((*it)[1].str());\n",
    "            const std::string content = (*it)[2].str();\n",
    "            g_sp.add_piece_with_id(id, content);\n",
    "            if (id > max_added_id) max_added_id = id;\n",
    "        }\n",
    "    } else {\n",
    "        cerr << \"Warning: tokenizer_config.json not found; cannot merge added tokens.\" << endl;\n",
    "    }\n",
    "\n",
    "    // Determine pad_id. For T5-style tokenizers, pad is usually after extra_ids, but we also try to look it up by string.\n",
    "    int64_t pad_id = -1;\n",
    "    if (!pad_token_str.empty()) {\n",
    "        pad_id = g_sp.id_for_piece(pad_token_str);\n",
    "    }\n",
    "    if (pad_id < 0 && extra_ids > 0) {\n",
    "        pad_id = g_base_sp_vocab_size + extra_ids;\n",
    "    }\n",
    "    if (pad_id < 0) {\n",
    "        // Fallback: keep 0, but warn (0 is usually <unk> for T5).\n",
    "        pad_id = 0;\n",
    "        cerr << \"Warning: could not determine pad_id; defaulting to 0 (this may be wrong for T5 tokenizers).\" << endl;\n",
    "    }\n",
    "    g_pad_id = pad_id;\n",
    "\n",
    "    // Effective vocab size is max id + 1 after merging (or base size if no added tokens).\n",
    "    g_vocab_size = static_cast<int64_t>(g_sp.vocab_size());\n",
    "    if (g_vocab_size < g_base_sp_vocab_size) g_vocab_size = g_base_sp_vocab_size;\n",
    "    cout << \"Tokenizer effective_vocab_size=\" << g_vocab_size << \" (base=\" << g_base_sp_vocab_size << \")\" << endl;\n",
    "    cout << \"Tokenizer pad_id=\" << g_pad_id << endl;\n",
    "\n",
    "    // --- Prepare model (initialized here) ---\n",
    "    const int64_t vocab_size = g_vocab_size;\n",
    "    if (vocab_size <= 0) throw std::runtime_error(\"Invalid vocab_size from tokenizer assets\");\n",
    "\n",
    "    g_device = torch::kCPU;\n",
    "    if (torch::cuda::is_available()) {\n",
    "        cout << \"CUDA available. Using GPU.\" << endl;\n",
    "        g_device = torch::Device(torch::kCUDA);\n",
    "    }\n",
    "\n",
    "    const int64_t d_model = 512;\n",
    "    const int64_t num_heads = 8;\n",
    "    const int64_t enc_layers = 1;\n",
    "    const int64_t dec_layers = 1;\n",
    "    const int64_t d_ff = 1024;\n",
    "    const int64_t model_max_len = 512;\n",
    "\n",
    "    g_t5 = torchlm::T5Model(vocab_size, d_model, num_heads, enc_layers, dec_layers, d_ff, model_max_len, 0.1);\n",
    "    g_t5->to(g_device);\n",
    "    g_model_ready = true;\n",
    "\n",
    "    cout << \"\\nModel ready. Total params: \" << model_summary::count_total_params(*g_t5) << \"\\n\";\n",
    "    model_summary::print_model_summary(*g_t5, model_summary::SummaryOptions{.print_each_param=false, .max_groups=60});\n",
    "\n",
    "    // --- Load dataset splits (train/validation/test) ---\n",
    "    cout << \"\\nLoading dataset splits from: \" << g_dataset_repo << endl;\n",
    "    const auto splits = hf_dataset::list_splits(g_dataset_repo, g_hf_token);\n",
    "    cout << \"Splits: \";\n",
    "    for (size_t i = 0; i < splits.size(); ++i) cout << splits[i] << (i + 1 == splits.size() ? \"\\n\" : \", \");\n",
    "\n",
    "    const size_t preview_train = 32;\n",
    "    const size_t preview_valid = 16;\n",
    "    const size_t preview_test  = 16;\n",
    "    g_train_tbl = hf_dataset::load_rows_split(g_dataset_repo, \"train\", 0, preview_train, g_hf_token);\n",
    "    g_valid_tbl = hf_dataset::load_rows_split(g_dataset_repo, \"validation\", 0, preview_valid, g_hf_token);\n",
    "    g_test_tbl  = hf_dataset::load_rows_split(g_dataset_repo, \"test\", 0, preview_test, g_hf_token);\n",
    "    g_dataset_loaded = true;\n",
    "\n",
    "    cout << \"\\nTrain preview:\" << endl;\n",
    "    hf_dataset::print_columns(g_train_tbl);\n",
    "    hf_dataset::print_head(g_train_tbl, 5);\n",
    "\n",
    "} catch (const std::exception& e) {\n",
    "    cerr << \"Error in Cell 2: \" << e.what() << endl;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77f505",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "using namespace std;\n",
    "using namespace mcppfa;\n",
    "\n",
    "// --- Training params (edit these) ---\n",
    "// NOTE: g_pad_id is derived from tokenizer_config.json in Cell 2.\n",
    "g_max_len = 512;\n",
    "g_lr = 2e-5;\n",
    "g_epochs = 100;\n",
    "g_batch_size = 64;\n",
    "\n",
    "// How many rows to fetch for each split\n",
    "g_n_train = 8;\n",
    "g_n_valid = 4;\n",
    "g_n_test  = 4;\n",
    "\n",
    "cout << \"Training params\" << endl;\n",
    "cout << \"  max_len=\" << g_max_len << \" pad_id=\" << g_pad_id << endl;\n",
    "cout << \"  lr=\" << g_lr << \" epochs=\" << g_epochs << \" batch_size=\" << g_batch_size << endl;\n",
    "cout << \"  n_train=\" << g_n_train << \" n_valid=\" << g_n_valid << \" n_test=\" << g_n_test << endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0bc01b",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "using namespace std;\n",
    "using namespace mcppfa;\n",
    "\n",
    "try {\n",
    "    if (!g_sp_loaded || !g_model_ready) {\n",
    "        throw std::runtime_error(\"Tokenizer/model not ready. Run Cell 1 first.\");\n",
    "    }\n",
    "    if (g_n_train == 0 || g_n_valid == 0 || g_n_test == 0) {\n",
    "        throw std::runtime_error(\"Invalid dataset sizes. Set g_n_train/g_n_valid/g_n_test in Cell 2.\");\n",
    "    }\n",
    "    if (g_max_len <= 0) throw std::runtime_error(\"g_max_len must be > 0\");\n",
    "    if (g_batch_size <= 0) throw std::runtime_error(\"g_batch_size must be > 0\");\n",
    "\n",
    "    // datasets-server often enforces a max page size; fetch in pages to be safe.\n",
    "    auto fetch_paged = [&](const std::string& split, size_t total) -> hf_dataset::Table {\n",
    "        const size_t page = 64;\n",
    "        hf_dataset::Table out;\n",
    "        size_t offset = 0;\n",
    "        while (offset < total) {\n",
    "            const size_t len = std::min(page, total - offset);\n",
    "            auto chunk = hf_dataset::load_rows_split(g_dataset_repo, split, offset, len, g_hf_token);\n",
    "            if (out.columns.empty()) out.columns = chunk.columns;\n",
    "            for (auto& r : chunk.rows) out.rows.push_back(std::move(r));\n",
    "            offset += len;\n",
    "        }\n",
    "        return out;\n",
    "    };\n",
    "\n",
    "    // Avoid re-downloading if the requested sizes are already loaded in-memory.\n",
    "    static size_t s_loaded_train = 0;\n",
    "    static size_t s_loaded_valid = 0;\n",
    "    static size_t s_loaded_test  = 0;\n",
    "    const bool need_fetch = (!g_dataset_loaded) || (s_loaded_train != g_n_train) || (s_loaded_valid != g_n_valid) || (s_loaded_test != g_n_test);\n",
    "    if (need_fetch) {\n",
    "        std::printf(\"Fetching dataset rows for training (paged)...\\n\");\n",
    "        g_train_tbl = fetch_paged(\"train\", g_n_train);\n",
    "        g_valid_tbl = fetch_paged(\"validation\", g_n_valid);\n",
    "        g_test_tbl  = fetch_paged(\"test\", g_n_test);\n",
    "        g_dataset_loaded = true;\n",
    "        s_loaded_train = g_n_train;\n",
    "        s_loaded_valid = g_n_valid;\n",
    "        s_loaded_test  = g_n_test;\n",
    "    } else {\n",
    "        std::printf(\"Dataset already loaded in-memory; skipping fetch.\\n\");\n",
    "    }\n",
    "\n",
    "    TrainingArguments args;\n",
    "    args.max_len = g_max_len;\n",
    "    args.pad_id = g_pad_id;\n",
    "    args.batch_size = g_batch_size;\n",
    "    args.epochs = g_epochs;\n",
    "    args.lr = g_lr;\n",
    "    args.input_col = \"state\";\n",
    "    args.label_col = \"very_short_result\";\n",
    "    args.one_token_target = true;\n",
    "    args.device = g_device;\n",
    "\n",
    "    Trainer<torchlm::T5Model> trainer(g_t5, g_sp, args);\n",
    "    trainer.set_splits(g_train_tbl, g_valid_tbl, g_test_tbl);\n",
    "\n",
    "    std::printf(\"Tokenizing splits once (this saves lots of time per epoch)...\\n\");\n",
    "    trainer.tokenize_splits_once();\n",
    "    trainer.train();\n",
    "\n",
    "    const string out_model = \"t5_dataset_finetuned.pt\";\n",
    "    torch::serialize::OutputArchive archive;\n",
    "    g_t5->save(archive);\n",
    "    archive.save_to(out_model);\n",
    "    std::printf(\"Saved fine-tuned model to %s\\n\", out_model.c_str());\n",
    "\n",
    "} catch (const std::exception& e) {\n",
    "    std::fprintf(stderr, \"Error in training cell: %s\\n\", e.what());\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130092c",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "string text = \"STATE PLAY CARD ROUND 2 BID HISTORY e2 b7 me b9 e1 b0 mp b0 e2 b0 SCORE US 9 THEM 9 LEADING THEM POINTS me 3 e1 2 mp 0 e2 0 US 3 THEM 2 PLAYERS CARDS PC me C r2 C r4 C r10 C r13 C r14 PC e1 D r4 C r6 C r7 C r8 C r9 C r12 PC mp H r14 S r6 S r9 D r3 D r7 D r10 PC e2 S r3 S r7 D r2 D r6 D r14 C r5 SUITS COUNT H 1 S 4 D 7 C 11 BID COUNT bc5 CURRENT BID me b9 BID SUIT C TRUMP COUNT 11 STATE PLAY CARD TABLE COUNT d3 TABLE CARDS U r0 D r4 D r10 D r14 TABLE SUIT D MY CARDS H r2 H r6 S r4 S r13 D r5 D r13 C r3 C r11\";\n",
    "\n",
    "// do inference with the fine-tuned model\n",
    "try {\n",
    "    if (!g_sp_loaded || !g_model_ready) {\n",
    "        throw std::runtime_error(\"Tokenizer/model not ready. Run Cell 1 first.\");\n",
    "    }\n",
    "    const int64_t max_len = g_max_len;\n",
    "    const int64_t pad_id = g_pad_id;\n",
    "\n",
    "    auto encode_fixed = [&](const std::string& text) -> torch::Tensor {\n",
    "        std::vector<int64_t> ids = g_sp.encode(text);\n",
    "        if (static_cast<int64_t>(ids.size()) > max_len) {\n",
    "            ids.resize(static_cast<size_t>(max_len));\n",
    "        }\n",
    "        while (static_cast<int64_t>(ids.size()) < max_len) {\n",
    "            ids.push_back(pad_id);\n",
    "        }\n",
    "        auto t = torch::from_blob(ids.data(), {1, max_len}, torch::TensorOptions().dtype(torch::kInt64)).clone();\n",
    "        return t.to(g_device);\n",
    "    };\n",
    "\n",
    "    g_t5->eval();\n",
    "    torch::NoGradGuard ng;\n",
    "\n",
    "    auto input_ids = encode_fixed(text);\n",
    "    // Decoder length = 1, since we only want one output token\n",
    "    auto decoder_input_ids = torch::full({1, 1}, pad_id, torch::TensorOptions().dtype(torch::kInt64).device(g_device));\n",
    "\n",
    "    auto logits = g_t5->forward(input_ids, decoder_input_ids); // [1, 1, V]\n",
    "    auto pred_ids = logits.argmax(-1); // [1, 1]\n",
    "\n",
    "    // Decode ONLY the first predicted token\n",
    "    auto pred_cpu = pred_ids.to(torch::kCPU).contiguous();\n",
    "    const int64_t first_id = pred_cpu[0][0].item<int64_t>();\n",
    "    std::vector<int64_t> one_token;\n",
    "    one_token.push_back(first_id);\n",
    "    auto decoded = g_sp.decode(one_token);\n",
    "\n",
    "    cout << \"\\nInput text:\\n\" << text << endl;\n",
    "    cout << \"\\nPredicted output (1 token):\\n\" << decoded << endl;\n",
    "\n",
    "} catch (const std::exception& e) {\n",
    "    cerr << \"Error in Cell 4: \" << e.what() << endl;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaadd272",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b685ffa",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "string text = \"A40\";\n",
    "\n",
    "// encoding with SentencePiece\n",
    "try {\n",
    "    if (!g_sp_loaded) {\n",
    "        throw std::runtime_error(\"Tokenizer not ready. Run Cell 1 first.\");\n",
    "    }\n",
    "\n",
    "    auto ids = g_sp.encode(text);\n",
    "    cout << \"Input text: \" << text << endl;\n",
    "    cout << \"Token IDs: \";\n",
    "    for (size_t i = 0; i < ids.size(); ++i) {\n",
    "        cout << ids[i] << (i + 1 == ids.size() ? \"\\n\" : \", \");\n",
    "    }\n",
    "\n",
    "    auto decoded = g_sp.decode(ids);\n",
    "    cout << \"Decoded text: \" << decoded << endl;\n",
    "\n",
    "} catch (const std::exception& e) {\n",
    "    cerr << \"Error in Cell 5: \" << e.what() << endl;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62415b",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "// print tokenizer vocab (raw SentencePiece pieces + HF added tokens)\n",
    "try {\n",
    "    if (!g_sp_loaded) {\n",
    "        throw std::runtime_error(\"Tokenizer not ready. Run Cell 2 first.\");\n",
    "    }\n",
    "\n",
    "    const int64_t limit = std::min<int64_t>(200, static_cast<int64_t>(g_sp.vocab_size()));\n",
    "    cout << \"\\nTokenizer vocabulary (first \" << limit << \" pieces):\" << endl;\n",
    "    for (int64_t i = 0; i < limit; ++i) {\n",
    "        string piece = g_sp.piece_for_id(i);\n",
    "        cout << \"ID \" << setw(4) << i << \": '\" << piece << \"'\" << endl;\n",
    "    }\n",
    "    cout << \"\\nTokenizer effective vocab_size=\" << g_sp.vocab_size() << \" (model uses \" << g_vocab_size << \")\" << endl;\n",
    "    cout << \"pad_id=\" << g_pad_id << endl;\n",
    "\n",
    "} catch (const std::exception& e) {\n",
    "    cerr << \"Error in Cell 8: \" << e.what() << endl;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a5905",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++17",
   "name": "xcpp17"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
