{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a67420",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# AG News Classification (Pure C++ / LibTorch / xcpp17)\n",
    "\n",
    "This notebook trains a **BiLSTM** text classifier on the **AG News** dataset using **pure C++ (LibTorch)** inside the **xcpp17** Jupyter kernel (xeus-cling).\n",
    "\n",
    "Kaggle link: https://www.kaggle.com/code/ishandutta/ag-news-classification-lstm\n",
    "\n",
    "**Dataset**: `dataset/train.csv` (120,000 rows) and `dataset/test.csv` (7,600 rows).\n",
    "\n",
    "**Columns**: `Class Index` (1..4), `Title`, `Description`. We combine `Title + \" \" + Description` into a single text field and map labels to **0..3**.\n",
    "\n",
    "## What you get\n",
    "\n",
    "- Training + evaluation in C++ (no Python ML stack)\n",
    "- Tokenization + padding (word-level) using `mcppfa::text::WordVocab`\n",
    "- Model checkpoint saving (`.pt`) + vocab export for reuse\n",
    "- Simple demo predictions + confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca920a9e",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "// --- LibTorch (repo-local, Linux/WSL build) ---\n",
    "#pragma cling add_include_path(\"./libtorch/include\")\n",
    "#pragma cling add_include_path(\"./libtorch/include/torch/csrc/api/include\")\n",
    "#pragma cling add_library_path(\"./libtorch/lib\")\n",
    "#pragma cling load(\"./libtorch/lib/libc10.so\")\n",
    "#pragma cling load(\"./libtorch/lib/libtorch_cpu.so\")\n",
    "#pragma cling load(\"./libtorch/lib/libtorch.so\")\n",
    "\n",
    "// --- Repo headers ---\n",
    "#pragma cling add_include_path(\"./include\")\n",
    "\n",
    "#include <torch/torch.h>\n",
    "\n",
    "#include <bits/stdc++.h>\n",
    "\n",
    "// Reusable helpers moved into the library\n",
    "// (header-only, safe in xeus-cling)\n",
    "#include <mcppfa/csv.hpp>\n",
    "#include <mcppfa/word_vocab.hpp>\n",
    "#include <mcppfa/torch_lstm.hpp>\n",
    "\n",
    "using namespace std;\n",
    "namespace fs = filesystem;\n",
    "\n",
    "// Unbuffered output (statements are OK in this cell)\n",
    "cout.setf(ios::unitbuf);\n",
    "cerr.setf(ios::unitbuf);\n",
    "setvbuf(stdout, nullptr, _IONBF, 0);\n",
    "setvbuf(stderr, nullptr, _IONBF, 0);\n",
    "\n",
    "// Repo root used throughout the notebook\n",
    "static fs::path root = fs::current_path();\n",
    "cout << \"Repo root (cwd): \" << root << \"\\n\";\n",
    "cout << \"LibTorch version: \" << TORCH_VERSION << \"\\n\";\n",
    "cout << \"CUDA available: \" << (torch::cuda::is_available() ? \"yes\" : \"no\") << \"\\n\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f6b0d",
   "metadata": {},
   "source": [
    "## 1) Load AG News CSVs\n",
    "\n",
    "We load the repo-local CSVs from `dataset/`. Each row has:\n",
    "- class index (1..4)\n",
    "- title\n",
    "- description\n",
    "\n",
    "We combine `title + \" \" + description` and shift labels to `0..3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd1800",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "struct ExampleRow {\n",
    "    int64_t label{};      // 0..3\n",
    "    std::string text;     // title + description\n",
    "};\n",
    "\n",
    "const fs::path& g_repo_root = root;\n",
    "if (g_repo_root.empty()) {\n",
    "    throw std::runtime_error(\"Repo root not found. Run Cell 2 then Cell 3, and ensure you started Jupyter from inside WSL repo.\");\n",
    "}\n",
    "\n",
    "// Build paths without `operator/` (some cling builds choke on it).\n",
    "fs::path g_train_csv = \"/home/warawreh/MakeCPPFunAgain/notebooks/AG News Classification/archive/train.csv\";\n",
    "\n",
    "fs::path g_test_csv = \"/home/warawreh/MakeCPPFunAgain/notebooks/AG News Classification/archive/test.csv\";\n",
    "\n",
    "// Optional: limit rows to iterate faster while you tune hyperparams.\n",
    "// If your kernel crashes, keep these small first (e.g. 20000 / 2000), then increase.\n",
    "int64_t g_limit_train = 120000;\n",
    "int64_t g_limit_test  = 8000;\n",
    "\n",
    "// Stream parse (avoids allocating a giant rows[][] first).\n",
    "auto load_ag_news_csv = [&](const fs::path& path, int64_t limit, size_t reserve_hint) -> std::vector<ExampleRow> {\n",
    "    std::ifstream in(path.string());\n",
    "    if (!in) throw std::runtime_error(\"Failed to open CSV: \" + path.string());\n",
    "\n",
    "    std::vector<ExampleRow> out;\n",
    "    if (limit > 0) out.reserve(static_cast<size_t>(limit));\n",
    "    else out.reserve(reserve_hint);\n",
    "\n",
    "    std::string line;\n",
    "    bool first = true;\n",
    "    while (std::getline(in, line)) {\n",
    "        if (!line.empty() && line.back() == '\\r') line.pop_back();\n",
    "        if (first) {\n",
    "            first = false;\n",
    "            continue; // header\n",
    "        }\n",
    "        if (line.empty()) continue;\n",
    "\n",
    "        auto r = mcppfa::csv::split_csv_line(line);\n",
    "        if (r.size() < 3) continue;\n",
    "        const int64_t cls = std::stoll(r[0]);\n",
    "        const int64_t label = cls - 1; // 1..4 -> 0..3\n",
    "        if (label < 0 || label > 3) continue;\n",
    "\n",
    "        std::string text = r[1];\n",
    "        text.push_back(' ');\n",
    "        text += r[2];\n",
    "\n",
    "        out.push_back(ExampleRow{label, std::move(text)});\n",
    "        if (limit > 0 && static_cast<int64_t>(out.size()) >= limit) break;\n",
    "    }\n",
    "\n",
    "    return out;\n",
    "};\n",
    "\n",
    "std::vector<ExampleRow> g_train_rows = load_ag_news_csv(g_train_csv, g_limit_train, 120000u);\n",
    "std::vector<ExampleRow> g_test_rows  = load_ag_news_csv(g_test_csv,  g_limit_test,  7600u);\n",
    "\n",
    "cout << \"Train rows: \" << g_train_rows.size() << \"\\n\";\n",
    "cout << \"Test rows : \" << g_test_rows.size() << \"\\n\";\n",
    "\n",
    "// Class distribution check\n",
    "std::array<int64_t, 4> train_counts{0,0,0,0};\n",
    "std::array<int64_t, 4> test_counts{0,0,0,0};\n",
    "for (const auto& r : g_train_rows) train_counts[static_cast<size_t>(r.label)]++;\n",
    "for (const auto& r : g_test_rows) test_counts[static_cast<size_t>(r.label)]++;\n",
    "cout << \"Train label counts (0..3): \";\n",
    "for (auto c : train_counts) cout << c << \" \";\n",
    "cout << \"\\n\";\n",
    "cout << \"Test  label counts (0..3): \";\n",
    "for (auto c : test_counts) cout << c << \" \";\n",
    "cout << \"\\n\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820267a",
   "metadata": {},
   "source": [
    "## 2) Tokenize + build vocabulary + pad sequences\n",
    "\n",
    "We use the repo’s simple word tokenizer (`mcppfa::text::word_tokenize_lower_ascii`) and a frequency-based vocab (`mcppfa::text::WordVocab`).\n",
    "\n",
    "Then we encode each example into a fixed-length `max_len` vector of token ids (padding with `<pad>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba5ce4",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "// Hyperparameters (roughly matching the Python baseline)\n",
    "int64_t g_vocab_size = 10000;\n",
    "int64_t g_embed_size = 32;\n",
    "int64_t g_max_len    = 200;  // capped for practicality (Python example used dataset max, which can get huge)\n",
    "\n",
    "// Build vocab from training texts only (string_view avoids duplicating all texts in memory)\n",
    "std::vector<std::string_view> g_train_text_views;\n",
    "g_train_text_views.reserve(g_train_rows.size());\n",
    "for (const auto& r : g_train_rows) g_train_text_views.emplace_back(r.text);\n",
    "\n",
    "mcppfa::text::WordVocab g_vocab(g_vocab_size);\n",
    "g_vocab.build_from_texts(g_train_text_views);\n",
    "cout << \"Built vocab. actual_size=\" << g_vocab.size() << \" (requested=\" << g_vocab_size << \")\\n\";\n",
    "\n",
    "// Light token-length stats (sampled; full scan can be slow/fragile in xeus-cling)\n",
    "{\n",
    "    const int64_t k = std::min<int64_t>(200, static_cast<int64_t>(g_train_text_views.size()));\n",
    "    int64_t max_tokens_seen = 0;\n",
    "    int64_t sum_tokens = 0;\n",
    "    for (int64_t i = 0; i < k; ++i) {\n",
    "        const auto n = static_cast<int64_t>(mcppfa::text::word_tokenize_lower_ascii(\n",
    "            g_train_text_views[static_cast<size_t>(i)]).size());\n",
    "        max_tokens_seen = std::max(max_tokens_seen, n);\n",
    "        sum_tokens += n;\n",
    "    }\n",
    "    cout << \"Train token length (sampled \" << k << \"): max=\" << max_tokens_seen\n",
    "         << \", avg=\" << (k == 0 ? 0.0 : (double)sum_tokens / (double)k)\n",
    "         << \"\\n\";\n",
    "}\n",
    "\n",
    "// Build tensors inline (avoid defining helper functions; cling can get picky if scope is corrupted).\n",
    "const int64_t n_train = static_cast<int64_t>(g_train_rows.size());\n",
    "auto g_x_train = torch::empty({n_train, g_max_len}, torch::TensorOptions().dtype(torch::kInt64));\n",
    "auto g_y_train = torch::empty({n_train},          torch::TensorOptions().dtype(torch::kInt64));\n",
    "{\n",
    "    auto xa = g_x_train.accessor<int64_t, 2>();\n",
    "    auto ya = g_y_train.accessor<int64_t, 1>();\n",
    "    for (int64_t i = 0; i < n_train; ++i) {\n",
    "        const auto& ex = g_train_rows[static_cast<size_t>(i)];\n",
    "        ya[i] = ex.label;\n",
    "        g_vocab.encode_padded_to(ex.text, &xa[i][0], g_max_len);\n",
    "    }\n",
    "}\n",
    "\n",
    "const int64_t n_test = static_cast<int64_t>(g_test_rows.size());\n",
    "auto g_x_test = torch::empty({n_test, g_max_len}, torch::TensorOptions().dtype(torch::kInt64));\n",
    "auto g_y_test = torch::empty({n_test},          torch::TensorOptions().dtype(torch::kInt64));\n",
    "{\n",
    "    auto xa = g_x_test.accessor<int64_t, 2>();\n",
    "    auto ya = g_y_test.accessor<int64_t, 1>();\n",
    "    for (int64_t i = 0; i < n_test; ++i) {\n",
    "        const auto& ex = g_test_rows[static_cast<size_t>(i)];\n",
    "        ya[i] = ex.label;\n",
    "        g_vocab.encode_padded_to(ex.text, &xa[i][0], g_max_len);\n",
    "    }\n",
    "}\n",
    "\n",
    "cout << \"X_train: \" << g_x_train.sizes() << \", y_train: \" << g_y_train.sizes() << \"\\n\";\n",
    "cout << \"X_test : \" << g_x_test.sizes()  << \", y_test : \" << g_y_test.sizes()  << \"\\n\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962c314",
   "metadata": {},
   "source": [
    "## 3) Define the BiLSTM classifier (LibTorch)\n",
    "\n",
    "This matches the architecture used in your C++ CLI implementation (`src/ag_news_lstm.cpp`):\n",
    "- Embedding\n",
    "- BiLSTM(128) + BiLSTM(64)\n",
    "- GlobalMaxPool over time\n",
    "- MLP + Dropout\n",
    "- 4-class logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d8422",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "// Notebook-local AG News model that *uses* the reusable LSTM wrapper from the library.\n",
    "// This keeps the library generic while making the notebook model easy to tweak.\n",
    "\n",
    "struct AgNewsLSTMClassifierImpl : torch::nn::Module {\n",
    "    torch::nn::Embedding emb{nullptr};\n",
    "    mcppfa::LSTMBlock lstm1{nullptr};\n",
    "    mcppfa::LSTMBlock lstm2{nullptr};\n",
    "\n",
    "    torch::nn::Linear fc1{nullptr};\n",
    "    torch::nn::Linear fc2{nullptr};\n",
    "    torch::nn::Linear fc3{nullptr};\n",
    "    torch::nn::Dropout drop{nullptr};\n",
    "    torch::nn::Linear out{nullptr};\n",
    "\n",
    "    AgNewsLSTMClassifierImpl(\n",
    "        int64_t vocab_size,\n",
    "        int64_t embed_size,\n",
    "        int64_t lstm1_hidden = 128,\n",
    "        int64_t lstm2_hidden = 64,\n",
    "        int64_t fc1_dim = 256,\n",
    "        int64_t fc2_dim = 128,\n",
    "        int64_t fc3_dim = 64,\n",
    "        double dropout = 0.25,\n",
    "        int64_t num_classes = 4,\n",
    "        bool bidirectional = true) {\n",
    "\n",
    "        if (vocab_size <= 0) throw std::runtime_error(\"AgNewsLSTMClassifier: vocab_size must be > 0\");\n",
    "        if (embed_size <= 0) throw std::runtime_error(\"AgNewsLSTMClassifier: embed_size must be > 0\");\n",
    "        if (lstm1_hidden <= 0 || lstm2_hidden <= 0) throw std::runtime_error(\"AgNewsLSTMClassifier: LSTM hidden sizes must be > 0\");\n",
    "        if (fc1_dim <= 0 || fc2_dim <= 0 || fc3_dim <= 0) throw std::runtime_error(\"AgNewsLSTMClassifier: FC dims must be > 0\");\n",
    "        if (!(dropout >= 0.0 && dropout < 1.0)) throw std::runtime_error(\"AgNewsLSTMClassifier: dropout must be in [0,1)\");\n",
    "        if (num_classes <= 1) throw std::runtime_error(\"AgNewsLSTMClassifier: num_classes must be > 1\");\n",
    "\n",
    "        emb = register_module(\"emb\", torch::nn::Embedding(torch::nn::EmbeddingOptions(vocab_size, embed_size)));\n",
    "\n",
    "        lstm1 = register_module(\"lstm1\", mcppfa::LSTMBlock(mcppfa::LSTMConfig{\n",
    "            .input_size = embed_size,\n",
    "            .hidden_size = lstm1_hidden,\n",
    "            .num_layers = 1,\n",
    "            .batch_first = true,\n",
    "            .bidirectional = bidirectional,\n",
    "            .dropout = 0.0\n",
    "        }));\n",
    "\n",
    "        lstm2 = register_module(\"lstm2\", mcppfa::LSTMBlock(mcppfa::LSTMConfig{\n",
    "            .input_size = lstm1->output_size(),\n",
    "            .hidden_size = lstm2_hidden,\n",
    "            .num_layers = 1,\n",
    "            .batch_first = true,\n",
    "            .bidirectional = bidirectional,\n",
    "            .dropout = 0.0\n",
    "        }));\n",
    "\n",
    "        const int64_t features = lstm2->output_size();\n",
    "        fc1 = register_module(\"fc1\", torch::nn::Linear(features, fc1_dim));\n",
    "        fc2 = register_module(\"fc2\", torch::nn::Linear(fc1_dim, fc2_dim));\n",
    "        fc3 = register_module(\"fc3\", torch::nn::Linear(fc2_dim, fc3_dim));\n",
    "        drop = register_module(\"drop\", torch::nn::Dropout(dropout));\n",
    "        out  = register_module(\"out\",  torch::nn::Linear(fc3_dim, num_classes));\n",
    "    }\n",
    "\n",
    "    // input_ids: [B, T] int64\n",
    "    torch::Tensor forward(torch::Tensor input_ids) {\n",
    "        auto x = emb->forward(input_ids);         // [B, T, E]\n",
    "        x = lstm1->forward(x);                   // [B, T, H1*dir]\n",
    "        x = lstm2->forward(x);                   // [B, T, H2*dir]\n",
    "\n",
    "        // GlobalMaxPooling1D over time dimension\n",
    "        x = std::get<0>(x.max(1));               // [B, H2*dir]\n",
    "\n",
    "        x = torch::relu(fc1->forward(x));\n",
    "        x = drop->forward(x);\n",
    "        x = torch::relu(fc2->forward(x));\n",
    "        x = drop->forward(x);\n",
    "        x = torch::relu(fc3->forward(x));\n",
    "        x = drop->forward(x);\n",
    "        return out->forward(x);                  // logits [B, C]\n",
    "    }\n",
    "};\n",
    "TORCH_MODULE(AgNewsLSTMClassifier);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe14b2f",
   "metadata": {},
   "source": [
    "## 4) Train + evaluate (with checkpoint saving)\n",
    "\n",
    "We train on the full training set and evaluate on the provided test set each epoch.\n",
    "\n",
    "Artifacts written to the notebook working directory:\n",
    "- `ag_news_lstm_xcpp17.pt` (best model weights)\n",
    "- `ag_news_vocab.txt` (one token per line; id==line index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f06561",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "// Device\n",
    "torch::Device device(torch::kCPU);\n",
    "cout << \"Using device: \" << (device.is_cuda() ? \"CUDA\" : \"CPU\") << \"\\n\";\n",
    "\n",
    "// Model (notebook-local, but built from reusable mcppfa::LSTMBlock)\n",
    "auto model = AgNewsLSTMClassifier(\n",
    "    g_vocab.size(),\n",
    "    g_embed_size,\n",
    "    /*lstm1_hidden=*/128,\n",
    "    /*lstm2_hidden=*/64,\n",
    "    /*fc1_dim=*/256,\n",
    "    /*fc2_dim=*/128,\n",
    "    /*fc3_dim=*/64,\n",
    "    /*dropout=*/0.25,\n",
    "    /*num_classes=*/4,\n",
    "    /*bidirectional=*/true);\n",
    "model->to(device);\n",
    "\n",
    "// Optimizer / loss\n",
    "torch::optim::Adam optimizer(model->parameters(), torch::optim::AdamOptions(1e-3));\n",
    "auto criterion = torch::nn::CrossEntropyLoss();\n",
    "\n",
    "// Training settings\n",
    "int64_t epochs = 2;\n",
    "int64_t batch_size = 256;\n",
    "int64_t log_every = 100;\n",
    "\n",
    "// Split settings (train -> train/val)\n",
    "const double val_fraction = 0.10;\n",
    "const int64_t split_seed = 123;\n",
    "\n",
    "// Checkpoint path + best tracker (persist for later cells)\n",
    "fs::path best_model_path = fs::current_path();\n",
    "best_model_path /= \"ag_news_lstm_xcpp17.pt\";\n",
    "double best_val_acc = -1.0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bc97d",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "// Prepare tensors\n",
    "auto Xall = g_x_train.to(device);\n",
    "auto Yall = g_y_train.to(device);\n",
    "auto Xtest = g_x_test.to(device);\n",
    "auto Ytest = g_y_test.to(device);\n",
    "\n",
    "const int64_t n_all = Xall.size(0);\n",
    "const int64_t n_val = std::max<int64_t>(1, static_cast<int64_t>(std::llround((double)n_all * val_fraction)));\n",
    "const int64_t n_tr  = n_all - n_val;\n",
    "cout << \"Train/Val split: train=\" << n_tr << \", val=\" << n_val << \" (from \" << n_all << \")\\n\";\n",
    "\n",
    "// Deterministic split\n",
    "torch::manual_seed(split_seed);\n",
    "auto perm_split = torch::randperm(n_all, torch::TensorOptions().dtype(torch::kInt64).device(device));\n",
    "auto idx_tr  = perm_split.slice(0, 0, n_tr);\n",
    "auto idx_val = perm_split.slice(0, n_tr, n_all);\n",
    "\n",
    "auto Xtr = Xall.index_select(0, idx_tr);\n",
    "auto Ytr = Yall.index_select(0, idx_tr);\n",
    "auto Xval = Xall.index_select(0, idx_val);\n",
    "auto Yval = Yall.index_select(0, idx_val);\n",
    "\n",
    "cout << \"Test samples (held out): \" << Xtest.size(0) << \"\\n\";\n",
    "\n",
    "for (int64_t epoch = 1; epoch <= epochs; ++epoch) {\n",
    "    const auto epoch_t0 = std::chrono::steady_clock::now();\n",
    "\n",
    "    model->train();\n",
    "    double train_loss_sum = 0.0;\n",
    "    double train_acc_sum  = 0.0;\n",
    "    int64_t steps = 0;\n",
    "\n",
    "    // Shuffle train indices each epoch\n",
    "    auto perm = torch::randperm(n_tr, torch::TensorOptions().dtype(torch::kInt64).device(device));\n",
    "    for (int64_t start = 0; start < n_tr; start += batch_size) {\n",
    "        auto end = std::min<int64_t>(start + batch_size, n_tr);\n",
    "        auto local = perm.slice(0, start, end);\n",
    "        auto idx = idx_tr.index_select(0, local);\n",
    "        auto xb = Xall.index_select(0, idx);\n",
    "        auto yb = Yall.index_select(0, idx);\n",
    "\n",
    "        optimizer.zero_grad();\n",
    "        auto logits = model->forward(xb);\n",
    "        auto loss = criterion(logits, yb);\n",
    "        loss.backward();\n",
    "        optimizer.step();\n",
    "\n",
    "        train_loss_sum += loss.item<double>();\n",
    "\n",
    "        auto preds = logits.detach().argmax(1);\n",
    "        train_acc_sum += preds.eq(yb).sum().item<double>() / (double)yb.size(0);\n",
    "\n",
    "        ++steps;\n",
    "        if (steps % log_every == 0) {\n",
    "            cout << \"epoch \" << epoch << \" step \" << steps\n",
    "                 << \" loss=\" << (train_loss_sum / (double)steps)\n",
    "                 << \" acc=\" << (train_acc_sum / (double)steps)\n",
    "                 << \"\\n\";\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Validation\n",
    "    model->eval();\n",
    "    torch::NoGradGuard no_grad;\n",
    "    double val_loss_sum = 0.0;\n",
    "    double val_acc_sum  = 0.0;\n",
    "    int64_t val_steps = 0;\n",
    "\n",
    "    const int64_t n_val_local = Xval.size(0);\n",
    "    for (int64_t start = 0; start < n_val_local; start += batch_size) {\n",
    "        auto end = std::min<int64_t>(start + batch_size, n_val_local);\n",
    "        auto xb = Xval.slice(0, start, end);\n",
    "        auto yb = Yval.slice(0, start, end);\n",
    "\n",
    "        auto logits = model->forward(xb);\n",
    "        auto loss = criterion(logits, yb);\n",
    "        val_loss_sum += loss.item<double>();\n",
    "\n",
    "        auto preds = logits.argmax(1);\n",
    "        val_acc_sum += preds.eq(yb).sum().item<double>() / (double)yb.size(0);\n",
    "\n",
    "        ++val_steps;\n",
    "    }\n",
    "\n",
    "    const double train_loss_epoch = (steps == 0) ? 0.0 : (train_loss_sum / (double)steps);\n",
    "    const double train_acc_epoch  = (steps == 0) ? 0.0 : (train_acc_sum  / (double)steps);\n",
    "    const double val_loss_epoch   = (val_steps == 0) ? 0.0 : (val_loss_sum / (double)val_steps);\n",
    "    const double val_acc_epoch    = (val_steps == 0) ? 0.0 : (val_acc_sum  / (double)val_steps);\n",
    "\n",
    "    const auto epoch_t1 = std::chrono::steady_clock::now();\n",
    "    const double epoch_sec = std::chrono::duration_cast<std::chrono::duration<double>>(epoch_t1 - epoch_t0).count();\n",
    "\n",
    "    cout << \"Epoch \" << epoch\n",
    "         << \" | train_loss=\" << train_loss_epoch\n",
    "         << \" train_acc=\" << train_acc_epoch\n",
    "         << \" | val_loss=\" << val_loss_epoch\n",
    "         << \" val_acc=\" << val_acc_epoch\n",
    "         << \" | time_sec=\" << epoch_sec\n",
    "         << \"\\n\";\n",
    "\n",
    "    if (val_steps > 0 && val_acc_epoch > best_val_acc) {\n",
    "        best_val_acc = val_acc_epoch;\n",
    "        torch::save(model, best_model_path.string());\n",
    "        cout << \"Saved best model to: \" << best_model_path\n",
    "             << \" (best_val_acc=\" << best_val_acc << \")\\n\";\n",
    "    }\n",
    "}\n",
    "\n",
    "cout << \"Training done. Best validation accuracy: \" << best_val_acc\n",
    "     << \" (checkpoint: \" << best_model_path << \")\\n\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363fda1",
   "metadata": {},
   "source": [
    "## 5) Load best checkpoint + run a small demo\n",
    "\n",
    "This mirrors the Python notebook’s demo but runs entirely in C++: we encode the input text with the same vocab, run the model, and print the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf4250",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "// Cell 13: Load the best checkpoint from Cell 11, then run demo predictions.\n",
    "// Requires running Cell 11 first (it defines `best_model_path`).\n",
    "\n",
    "if (fs::exists(best_model_path)) {\n",
    "    torch::load(model, best_model_path.string());\n",
    "    model->to(device);\n",
    "    cout << \"Loaded best model from: \" << best_model_path << \"\\n\";\n",
    "} else {\n",
    "    cout << \"Warning: best model file not found at: \" << best_model_path\n",
    "         << \" (using current in-memory weights)\\n\";\n",
    "}\n",
    "\n",
    "model->eval();\n",
    "torch::NoGradGuard ng2;\n",
    "\n",
    "static const std::vector<std::string> k_labels = {\n",
    "    \"World News\",\n",
    "    \"Sports News\",\n",
    "    \"Business News\",\n",
    "    \"Science-Technology News\",\n",
    "};\n",
    "\n",
    "auto predict_one = [&](const std::string& text) {\n",
    "    auto ids = g_vocab.encode_padded(text, g_max_len);\n",
    "    auto x = torch::from_blob(ids.data(), {1, g_max_len}, torch::TensorOptions().dtype(torch::kInt64))\n",
    "                 .clone()\n",
    "                 .to(device);\n",
    "    auto logits = model->forward(x);\n",
    "    auto pred = logits.argmax(1).to(torch::kCPU).item<int64_t>();\n",
    "    cout << \"\\\"\" << text << \"\\\" => \" << k_labels.at(static_cast<size_t>(pred)) << \"\\n\";\n",
    "};\n",
    "\n",
    "predict_one(\"New evidence of virus risks from wildlife trade\");\n",
    "predict_one(\"Coronavirus: Bank pumps £100bn into UK economy to aid recovery\");\n",
    "predict_one(\"Trump's bid to end Obama-era immigration policy ruled unlawful\");\n",
    "predict_one(\"David Luiz’s future with Arsenal to be decided this week\");\n",
    "predict_one(\"Indian Economic budget supports the underprivileged sections of society\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb6fc94",
   "metadata": {},
   "source": [
    "## 6) Full evaluation: confusion matrix + micro metrics\n",
    "\n",
    "This reproduces the Python notebook’s evaluation section (confusion matrix + micro precision/recall/accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef562f",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "// Cell 15: Confusion matrix + accuracy on the test set (true held-out test.csv).\n",
    "// Loads the best checkpoint (saved from validation in Cell 11) before evaluation.\n",
    "\n",
    "if (fs::exists(best_model_path)) {\n",
    "    torch::load(model, best_model_path.string());\n",
    "    model->to(device);\n",
    "    cout << \"Loaded best model from: \" << best_model_path << \"\\n\";\n",
    "} else {\n",
    "    cout << \"Warning: best model file not found at: \" << best_model_path\n",
    "         << \" (using current in-memory weights)\\n\";\n",
    "}\n",
    "\n",
    "model->eval();\n",
    "torch::NoGradGuard ng3;\n",
    "\n",
    "// Confusion matrix: rows = true label, cols = predicted label\n",
    "std::array<std::array<int64_t, 4>, 4> cm{};\n",
    "for (auto& row : cm) row.fill(0);\n",
    "\n",
    "const int64_t batch_size_eval = 256;\n",
    "const int64_t num_test = g_x_test.size(0);\n",
    "int64_t correct = 0;\n",
    "int64_t total = 0;\n",
    "\n",
    "for (int64_t start = 0; start < num_test; start += batch_size_eval) {\n",
    "    const int64_t end = std::min<int64_t>(start + batch_size_eval, num_test);\n",
    "    auto xb = g_x_test.slice(0, start, end).to(device);\n",
    "    auto yb = g_y_test.slice(0, start, end).to(device);\n",
    "\n",
    "    auto logits = model->forward(xb);\n",
    "    auto preds = logits.argmax(1);\n",
    "\n",
    "    auto preds_cpu = preds.to(torch::kCPU);\n",
    "    auto y_cpu = yb.to(torch::kCPU);\n",
    "\n",
    "    auto pa = preds_cpu.accessor<int64_t, 1>();\n",
    "    auto ya = y_cpu.accessor<int64_t, 1>();\n",
    "\n",
    "    for (int64_t i = 0; i < preds_cpu.size(0); ++i) {\n",
    "        const int64_t yt = ya[i];\n",
    "        const int64_t yp = pa[i];\n",
    "        if (0 <= yt && yt < 4 && 0 <= yp && yp < 4) {\n",
    "            cm[static_cast<size_t>(yt)][static_cast<size_t>(yp)]++;\n",
    "        }\n",
    "        if (yt == yp) correct++;\n",
    "        total++;\n",
    "    }\n",
    "}\n",
    "\n",
    "cout << \"Confusion matrix (rows=true, cols=pred):\\n\";\n",
    "cout << \"      0      1      2      3\\n\";\n",
    "for (int r = 0; r < 4; ++r) {\n",
    "    cout << r << \" \";\n",
    "    for (int c = 0; c < 4; ++c) {\n",
    "        cout << std::setw(6) << cm[static_cast<size_t>(r)][static_cast<size_t>(c)] << \" \";\n",
    "    }\n",
    "    cout << \"\\n\";\n",
    "}\n",
    "\n",
    "const double acc = total == 0 ? 0.0 : (double)correct / (double)total;\n",
    "cout << \"Test accuracy: \" << acc << \"\\n\";\n",
    "\n",
    "// Micro precision/recall equal accuracy for single-label multi-class, but we print both for clarity.\n",
    "cout << \"Micro precision: \" << acc << \"\\n\";\n",
    "cout << \"Micro recall   : \" << acc << \"\\n\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c9503",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++17",
   "name": "xcpp17"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
