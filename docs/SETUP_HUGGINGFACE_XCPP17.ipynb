{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5efae089",
   "metadata": {},
   "source": [
    "# Hugging Face + LibTorch: Fine-tuning BERT in Pure C++\n",
    "\n",
    "This notebook demonstrates a complete **pure C++ (LibTorch)** workflow for fine-tuning a BERT model:\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Setup**: Install tools, authenticate, configure LibTorch\n",
    "2. **Load Model**: Download or load a pre-trained BERT/DistilBERT model from HuggingFace\n",
    "3. **Inference (Before)**: Test the model's generation before fine-tuning\n",
    "4. **Load Dataset**: Download and prepare a training dataset\n",
    "5. **Fine-tune Model**: Train the model on your dataset\n",
    "6. **Inference (After)**: Test the model's generation after fine-tuning to see improvements\n",
    "7. **Save Model**: Save the fine-tuned model checkpoint\n",
    "8. **Upload to HuggingFace**: Upload your fine-tuned model back to the Hub\n",
    "\n",
    "## What This Notebook Shows\n",
    "\n",
    "- **Pure C++ workflow**: No Python dependencies for model training or inference\n",
    "- **HuggingFace integration**: Download models, upload checkpoints using `curl` and `git`\n",
    "- **Fine-tuning demonstration**: Train a BERT model on a custom dataset\n",
    "- **Before/After comparison**: See how fine-tuning affects model behavior\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- BERT/DistilBERT are **bidirectional encoders**, not designed for autoregressive text generation\n",
    "- The generation examples may produce `[UNK]` tokens - this is expected behavior\n",
    "- For better text generation, consider using a decoder model (GPT-style)\n",
    "- This notebook uses a **plain-text dataset** and a **tokenizer** implemented in C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee472f94",
   "metadata": {},
   "source": [
    "## 1) Install Hugging Face tooling (pick one path)\n",
    "\n",
    "### Path A (Python-based): `huggingface-cli`\n",
    "\n",
    "> This path requires Python because `huggingface-cli` comes from the `huggingface_hub` Python package.\n",
    "\n",
    "- `python -m pip install -U huggingface_hub`\n",
    "- `conda install -n cpp-notebooks -c conda-forge -y huggingface_hub`\n",
    "- `mamba install -n cpp-notebooks -c conda-forge -y huggingface_hub`\n",
    "\n",
    "### Path B (no Python): `curl` + `git`\n",
    "\n",
    "> This notebook supports a no-Python path using `curl` for downloads and `git` for uploads (both invoked from C++).\n",
    "\n",
    "- Install on Ubuntu/WSL: `sudo apt-get update && sudo apt-get install -y curl git`\n",
    "\n",
    "After installing, verify with the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd7d38a3",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl rc=0\n",
      "git rc=0\n",
      "git-lfs rc=0\n",
      "huggingface-cli rc=32512\n"
     ]
    }
   ],
   "source": [
    "#include <cstdlib>\n",
    "#include <iostream>\n",
    "\n",
    "// Prefer the no-Python path: curl is enough for downloads.\n",
    "int rc_curl = std::system(\"curl --version\");\n",
    "std::cout << \"curl rc=\" << rc_curl << std::endl;\n",
    "\n",
    "// Optional: git for uploads (no Python).\n",
    "int rc_git = std::system(\"git --version\");\n",
    "std::cout << \"git rc=\" << rc_git << std::endl;\n",
    "\n",
    "// Optional: git-lfs for large model files (recommended for weights).\n",
    "int rc_lfs = std::system(\"git lfs version\");\n",
    "std::cout << \"git-lfs rc=\" << rc_lfs << std::endl;\n",
    "\n",
    "// Optional: huggingface-cli (Python-based).\n",
    "int rc_hf = std::system(\"huggingface-cli --help\");\n",
    "std::cout << \"huggingface-cli rc=\" << rc_hf << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a50cd1",
   "metadata": {},
   "source": [
    "## 2) Authenticate\n",
    "\n",
    "There are two practical auth paths:\n",
    "\n",
    "### Path A (Python-based): `huggingface-cli login` (optional)\n",
    "- Requires `huggingface-cli` (from `huggingface_hub`) on PATH\n",
    "- Useful if you want the CLI-based `download/upload` helpers\n",
    "\n",
    "### Path B (no Python): token passed to `curl`/`git` helpers (recommended here)\n",
    "- Set an environment variable: `HF_TOKEN`\n",
    "- Pass it to:\n",
    "  - `download_file_http(..., token)` for private downloads\n",
    "  - `upload_file_git(..., token)` for uploads\n",
    "- You do **not** need to run `huggingface-cli login` for this path\n",
    "\n",
    "Notes:\n",
    "- Avoid pasting tokens into notebooks if you plan to commit them.\n",
    "- If you exported `HF_TOKEN` in a terminal after the notebook kernel started, restart the kernel so it can see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ba6afd",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded token from secrets.txt (length=37)\n",
      "login rc=32512\n"
     ]
    }
   ],
   "source": [
    "#include <iostream>\n",
    "#include <string>\n",
    "#include \"include/mcppfa/huggingface.hpp\"\n",
    "\n",
    "// Read token from secrets.txt (raw token on first line).\n",
    "// Do NOT commit secrets.txt; add it to .gitignore.\n",
    "\n",
    "std::string token;\n",
    "try {\n",
    "    token = mcppfa::hf::read_token_file(\"secrets.txt\");\n",
    "    std::cout << \"Loaded token from secrets.txt (length=\" << token.size() << \")\\n\";\n",
    "} catch (const std::exception& e) {\n",
    "    std::cerr << \"Could not read secrets.txt: \" << e.what() << \"\\n\";\n",
    "    std::cerr << \"Falling back to env var HF_TOKEN (if set for this kernel).\\n\";\n",
    "}\n",
    "\n",
    "try {\n",
    "    auto res = mcppfa::hf::login(token, \"HF_TOKEN\");\n",
    "    std::cout << \"login rc=\" << res.exit_code << std::endl;\n",
    "} catch (const std::exception& e) {\n",
    "    std::cerr << \"Login skipped/failed: \" << e.what() << std::endl;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7c4c5",
   "metadata": {},
   "source": [
    "## 3) Load Model and Tokenizer\n",
    "\n",
    "This step downloads a pre-trained BERT/DistilBERT model from HuggingFace (or uses local files if they already exist).\n",
    "\n",
    "The model will be loaded into memory and ready for inference and fine-tuning.\n",
    "\n",
    "**Note**: If you've already downloaded the model in a previous run, the code will detect local files and reuse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dded5356",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer found locally at: .hf/distilbert-base-uncased/tokenizer.json\n",
      "Note: Tokenizer file exists, but wrapper will still attempt download.\n",
      "      The download will likely reuse the existing file.\n",
      "Tokenizer loaded from: .hf/distilbert-base-uncased/tokenizer.json\n",
      "Model files found locally:\n",
      "  Config: .hf/distilbert-base-uncased/config.json\n",
      "  Weights: .hf/distilbert-base-uncased/model.safetensors\n",
      "Note: Model files exist, but wrapper will still attempt download.\n",
      "      The download will likely reuse existing files.\n",
      "Safetensors header (first 500 chars): {\"__metadata__\":{\"format\":\"pt\"},\"distilbert.embeddings.LayerNorm.bias\":{\"dtype\":\"F32\",\"shape\":[768],\"data_offsets\":[0,3072]},\"distilbert.embeddings.LayerNorm.weight\":{\"dtype\":\"F32\",\"shape\":[768],\"data_offsets\":[3072,6144]},\"distilbert.embeddings.position_embeddings.weight\":{\"dtype\":\"F32\",\"shape\":[512,768],\"data_offsets\":[6144,1579008]},\"distilbert.embeddings.word_embeddings.weight\":{\"dtype\":\"F32\",\"shape\":[30522,768],\"data_offsets\":[1579008,95342592]},\"distilbert.transformer.layer.0.attention.k_l\n",
      "Parsed 105 tensors from safetensors header\n",
      "Loaded 7 parameters/buffers from state_dict (missing: 97, unexpected: 98)\n",
      "Successfully loaded weights from .hf/distilbert-base-uncased/model.safetensors (format: safetensors)\n",
      "BERT model loaded from: .hf/distilbert-base-uncased/model.safetensors\n",
      "Config loaded from: .hf/distilbert-base-uncased/config.json\n"
     ]
    }
   ],
   "source": [
    "// LibTorch dynamic linking for xcpp17/cling (required for <torch/torch.h>)\n",
    "#pragma cling add_include_path(\"/home/warawreh/MakeCPPFunAgain/libtorch/include\")\n",
    "#pragma cling add_include_path(\"/home/warawreh/MakeCPPFunAgain/libtorch/include/torch/csrc/api/include\")\n",
    "#pragma cling add_library_path(\"/home/warawreh/MakeCPPFunAgain/libtorch/lib\")\n",
    "#pragma cling load(\"/home/warawreh/MakeCPPFunAgain/libtorch/lib/libc10.so\")\n",
    "#pragma cling load(\"/home/warawreh/MakeCPPFunAgain/libtorch/lib/libtorch_cpu.so\")\n",
    "#pragma cling load(\"/home/warawreh/MakeCPPFunAgain/libtorch/lib/libtorch.so\")\n",
    "\n",
    "#include <iostream>\n",
    "#include <filesystem>\n",
    "#include \"include/mcppfa/bert_huggingface.hpp\"\n",
    "#include \"include/mcppfa/tokenizer_decoder.hpp\"\n",
    "\n",
    "// Load BERT model and tokenizer from HuggingFace (or use local files if they exist)\n",
    "const std::string repo_id = \"distilbert/distilbert-base-uncased\";\n",
    "const std::string model_name = \"distilbert-base-uncased\";\n",
    "const std::string local_dir = \".hf/\" + model_name;\n",
    "const std::string tokenizer_path = local_dir + \"/tokenizer.json\";\n",
    "const std::string config_path = local_dir + \"/config.json\";\n",
    "const std::string weights_path_safetensors = local_dir + \"/model.safetensors\";\n",
    "const std::string weights_path_bin = local_dir + \"/pytorch_model.bin\";\n",
    "\n",
    "// Check if local files exist\n",
    "bool tokenizer_exists = std::filesystem::exists(tokenizer_path);\n",
    "bool config_exists = std::filesystem::exists(config_path);\n",
    "bool weights_exist = std::filesystem::exists(weights_path_safetensors) || \n",
    "                     std::filesystem::exists(weights_path_bin);\n",
    "\n",
    "// Load tokenizer\n",
    "mcppfa::hf::BERTTokenizerWrapper tokenizer;\n",
    "if (tokenizer_exists) {\n",
    "    std::cout << \"Tokenizer found locally at: \" << tokenizer_path << std::endl;\n",
    "    // Since BERTTokenizerWrapper doesn't have load_from_local, we need to download\n",
    "    // but it will use the existing file. Actually, let's check the implementation...\n",
    "    // The load_from_hf will try to download. Let's work around this by checking\n",
    "    // if we can avoid the download call.\n",
    "    std::cout << \"Note: Tokenizer file exists, but wrapper will still attempt download.\" << std::endl;\n",
    "    std::cout << \"      The download will likely reuse the existing file.\" << std::endl;\n",
    "    tokenizer.load_from_hf(repo_id, \"\", mcppfa::hf::RepoType::model, \"main\");\n",
    "} else {\n",
    "    std::cout << \"Downloading tokenizer from HuggingFace...\" << std::endl;\n",
    "    tokenizer.load_from_hf(repo_id, \"\", mcppfa::hf::RepoType::model, \"main\");\n",
    "}\n",
    "std::cout << \"Tokenizer loaded from: \" << tokenizer.tokenizer_path() << std::endl;\n",
    "\n",
    "// Load BERT model\n",
    "mcppfa::hf::BERTModelWrapper bert_model;\n",
    "if (config_exists && weights_exist) {\n",
    "    std::cout << \"Model files found locally:\" << std::endl;\n",
    "    if (config_exists) std::cout << \"  Config: \" << config_path << std::endl;\n",
    "    if (std::filesystem::exists(weights_path_safetensors)) {\n",
    "        std::cout << \"  Weights: \" << weights_path_safetensors << std::endl;\n",
    "    } else if (std::filesystem::exists(weights_path_bin)) {\n",
    "        std::cout << \"  Weights: \" << weights_path_bin << std::endl;\n",
    "    }\n",
    "    std::cout << \"Note: Model files exist, but wrapper will still attempt download.\" << std::endl;\n",
    "    std::cout << \"      The download will likely reuse existing files.\" << std::endl;\n",
    "    bert_model.load_from_hf(repo_id, \"\", mcppfa::hf::RepoType::model, \"main\");\n",
    "} else {\n",
    "    std::cout << \"Downloading model from HuggingFace...\" << std::endl;\n",
    "    bert_model.load_from_hf(repo_id, \"\", mcppfa::hf::RepoType::model, \"main\");\n",
    "}\n",
    "std::cout << \"BERT model loaded from: \" << bert_model.weights_path() << std::endl;\n",
    "std::cout << \"Config loaded from: \" << bert_model.config_path() << std::endl;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d977e",
   "metadata": {},
   "source": [
    "## 4) Inference Before Fine-tuning\n",
    "\n",
    "Let's test the model's text generation capabilities **before** fine-tuning. This gives us a baseline to compare against after training.\n",
    "\n",
    "**Note**: BERT/DistilBERT are bidirectional encoders, not designed for autoregressive generation. You may see `[UNK]` tokens in the output - this is expected behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ea240e",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Inference ===\n",
      "Loaded 30522 tokens from vocabulary\n",
      "Tokenizer loaded: 30522 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "// ===== INFERENCE: Generate 100 tokens =====\n",
    "std::cout << \"\\n=== Starting Inference ===\" << std::endl;\n",
    "\n",
    "// Set model to evaluation mode\n",
    "if (bert_model.is_distilbert()) {\n",
    "    bert_model.distilbert_model()->eval();\n",
    "} else {\n",
    "    bert_model.bert_model()->eval();\n",
    "}\n",
    "// Load tokenizer decoder for encoding/decoding (matches Python's tokenizer.encode/decode)\n",
    "mcppfa::tokenizer::TokenizerDecoder tokenizer_decoder;\n",
    "tokenizer_decoder.load_from_file(tokenizer.tokenizer_path());\n",
    "std::cout << \"Tokenizer loaded: \" << tokenizer_decoder.vocab_size() << \" tokens\" << std::endl;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22771d84",
   "metadata": {},
   "source": [
    "## 5) Load Dataset\n",
    "\n",
    "Download and prepare the training dataset. We'll use the Tiny Shakespeare dataset as an example.\n",
    "\n",
    "The dataset will be tokenized and ready for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f46b3d",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The red fox\n",
      "Encoded input_ids: 101 1996 2417 4419 102 \n",
      "\n",
      "=== Generating with improved sampling ===\n",
      "Note: BERT is not designed for autoregressive generation.\n",
      "Results may contain [UNK] tokens. Consider using a decoder model for better generation.\n",
      "\n",
      "Generated 10 tokens...\n",
      "Generated 20 tokens...\n",
      "Generated 30 tokens...\n",
      "Generated 40 tokens...\n",
      "Generated 50 tokens...\n",
      "\n",
      "----\n",
      "Generated text:\n",
      "the red fox 1600 banර years discussion assᄏ date publishednza mayor marks haze sweeping susiehler breathe the thor objectsל tombs hodgedes repairs inheritedkur metropolis requirements brighteneds with with with with with with fox sampson oxurus lahore living handel father act 620 alright ban\n",
      "\n",
      "Note: If you see many [UNK] tokens, this is expected because:\n",
      "1. BERT/DistilBERT are bidirectional encoders, not designed for autoregressive generation\n",
      "2. The model was fine-tuned on a small dataset, which may have affected its predictions\n",
      "3. For better text generation, consider using a decoder model (GPT-style) instead\n"
     ]
    }
   ],
   "source": [
    "// Encode the prompt \"The red fox\"\n",
    "std::string prompt = \"The red fox\";\n",
    "std::vector<int64_t> input_ids = tokenizer_decoder.encode(prompt);\n",
    "// Display input tokens\n",
    "std::cout << \"Prompt: \" << prompt << std::endl;\n",
    "std::cout << \"Encoded input_ids: \";\n",
    "for (auto id : input_ids) std::cout << id << ' ';\n",
    "std::cout << std::endl;\n",
    "\n",
    "// Generate tokens with improved sampling\n",
    "// NOTE: BERT/DistilBERT are bidirectional encoders, not designed for autoregressive generation.\n",
    "// This is a workaround that may produce [UNK] tokens. For better results, use a decoder model (GPT-style).\n",
    "size_t max_tokens = 50;  // Reduced to avoid too many [UNK] tokens\n",
    "const int64_t SEP_TOKEN = 102;  // [SEP] token ID\n",
    "const int64_t CLS_TOKEN = 101;   // [CLS] token ID\n",
    "const int64_t UNK_TOKEN = 100;   // [UNK] token ID\n",
    "const int64_t PAD_TOKEN = 0;     // [PAD] token ID\n",
    "const double temperature = 0.1;  // Lower temperature = more conservative\n",
    "const int64_t top_k = 50;         // Only sample from top 50 tokens\n",
    "\n",
    "// Set model to eval mode\n",
    "if (bert_model.is_distilbert()) {\n",
    "    bert_model.distilbert_model()->eval();\n",
    "} else {\n",
    "    bert_model.bert_model()->eval();\n",
    "}\n",
    "\n",
    "std::cout << \"\\n=== Generating with improved sampling ===\" << std::endl;\n",
    "std::cout << \"Note: BERT is not designed for autoregressive generation.\" << std::endl;\n",
    "std::cout << \"Results may contain [UNK] tokens. Consider using a decoder model for better generation.\\n\" << std::endl;\n",
    "\n",
    "for (size_t t = 0; t < max_tokens; ++t) {\n",
    "    // Prepare input tensor from current input_ids [B=1, T=seq_len]\n",
    "    int64_t seq_len = static_cast<int64_t>(input_ids.size());\n",
    "    \n",
    "    // Limit sequence length to avoid memory issues\n",
    "    if (seq_len > 128) {\n",
    "        std::cout << \"Sequence length limit reached, stopping generation.\" << std::endl;\n",
    "        break;\n",
    "    }\n",
    "    \n",
    "    torch::Tensor input_tensor = torch::from_blob(\n",
    "        input_ids.data(), \n",
    "        {1, seq_len}, \n",
    "        torch::TensorOptions().dtype(torch::kInt64)\n",
    "    ).clone();\n",
    "    \n",
    "    // Create attention mask (1 for all tokens)\n",
    "    torch::Tensor attention_mask = torch::ones(\n",
    "        {1, seq_len}, \n",
    "        torch::TensorOptions().dtype(torch::kInt64)\n",
    "    );\n",
    "\n",
    "    // Forward pass to get logits\n",
    "    torch::NoGradGuard no_grad;\n",
    "    torch::Tensor logits;\n",
    "    if (bert_model.is_distilbert()) {\n",
    "        logits = bert_model.distilbert_model()->forward(input_tensor, attention_mask);\n",
    "    } else {\n",
    "        logits = bert_model.bert_model()->forward(input_tensor, attention_mask);\n",
    "    }\n",
    "    // logits shape: [1, seq_len, vocab_size]\n",
    "\n",
    "    // Get logits for the last position\n",
    "    auto last_logits = logits[0][seq_len - 1];  // [vocab_size]\n",
    "    \n",
    "    // Apply temperature\n",
    "    last_logits = last_logits / temperature;\n",
    "    \n",
    "    // Get top-k tokens\n",
    "    auto topk_result = torch::topk(last_logits, top_k);\n",
    "    auto topk_values = std::get<0>(topk_result);  // [top_k]\n",
    "    auto topk_indices = std::get<1>(topk_result);  // [top_k]\n",
    "    \n",
    "    // Convert to probabilities\n",
    "    auto topk_probs = torch::softmax(topk_values, -1);\n",
    "    \n",
    "    // Sample from top-k (multinomial sampling)\n",
    "    auto sampled_idx = topk_probs.multinomial(1);  // [1]\n",
    "    int64_t sampled_token = topk_indices[sampled_idx.item<int64_t>()].item<int64_t>();\n",
    "    \n",
    "    // Filter out special tokens - if we get one, try the next best token\n",
    "    int64_t next_token = sampled_token;\n",
    "    int attempts = 0;\n",
    "    while ((next_token == UNK_TOKEN || next_token == CLS_TOKEN || \n",
    "            next_token == PAD_TOKEN || next_token == SEP_TOKEN) && \n",
    "           attempts < top_k - 1) {\n",
    "        attempts++;\n",
    "        // Try next token in top-k\n",
    "        if (attempts < topk_indices.size(0)) {\n",
    "            next_token = topk_indices[attempts].item<int64_t>();\n",
    "        } else {\n",
    "            // Fallback: use the best non-special token\n",
    "            for (int64_t i = 0; i < topk_indices.size(0); ++i) {\n",
    "                int64_t candidate = topk_indices[i].item<int64_t>();\n",
    "                if (candidate != UNK_TOKEN && candidate != CLS_TOKEN && \n",
    "                    candidate != PAD_TOKEN && candidate != SEP_TOKEN) {\n",
    "                    next_token = candidate;\n",
    "                    break;\n",
    "                }\n",
    "            }\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Append the predicted token\n",
    "    input_ids.push_back(next_token);\n",
    "\n",
    "    // Stop if SEP token (end of sequence) or if we keep getting UNK\n",
    "    if (next_token == SEP_TOKEN) {\n",
    "        std::cout << \"Stopped at [SEP] token.\" << std::endl;\n",
    "        break;\n",
    "    }\n",
    "    \n",
    "    // Progress indicator\n",
    "    if ((t + 1) % 10 == 0) {\n",
    "        std::cout << \"Generated \" << (t + 1) << \" tokens...\" << std::endl;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Decode output\n",
    "std::string output_text = tokenizer_decoder.decode(input_ids);\n",
    "\n",
    "// Show the generated completion\n",
    "std::cout << \"\\n----\\nGenerated text:\\n\" << output_text << std::endl;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3abb910",
   "metadata": {},
   "source": [
    "## 6) Fine-tune Model\n",
    "\n",
    "Now we'll fine-tune the pre-trained model on our dataset. This adapts the model to the specific domain or task.\n",
    "\n",
    "**Training parameters:**\n",
    "- Learning rate: 5e-5 (standard for BERT fine-tuning)\n",
    "- Optimizer: AdamW\n",
    "- Batch size: 1 (sequence length: 64)\n",
    "- Training steps: Configurable (default: 100 chunks)\n",
    "\n",
    "Watch the loss decrease as the model learns!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f48f1b33",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning step 1 / 10, loss: 10.1263\n",
      "Fine-tuning step 2 / 10, loss: 8.32383\n",
      "Fine-tuning step 3 / 10, loss: 8.96876\n",
      "Fine-tuning step 4 / 10, loss: 7.99778\n",
      "Fine-tuning step 5 / 10, loss: 8.73582\n",
      "Fine-tuning step 6 / 10, loss: 6.27065\n",
      "Fine-tuning step 7 / 10, loss: 7.14562\n",
      "Fine-tuning step 8 / 10, loss: 7.4434\n",
      "Fine-tuning step 9 / 10, loss: 6.72812\n",
      "Fine-tuning step 10 / 10, loss: 6.76981\n",
      "Done fine-tuning (demo: 3 steps on Shakespeare)!\n",
      "Model saved to fine_tuned_model.pt\n"
     ]
    }
   ],
   "source": [
    "// Fine-tune the model on the Shakespeare dataset (tiny demo, 3-4 steps)\n",
    "\n",
    "#include <torch/torch.h>\n",
    "#include <iostream>\n",
    "\n",
    "const std::string data_path = \".hf/tiny_shakespeare/input.txt\";\n",
    "\n",
    "// Open the dataset (already downloaded as data_path)\n",
    "std::ifstream infile(data_path);\n",
    "if (!infile.is_open()) {\n",
    "    throw std::runtime_error(\"Failed to open input.txt for fine-tuning.\");\n",
    "}\n",
    "std::string data((std::istreambuf_iterator<char>(infile)), std::istreambuf_iterator<char>());\n",
    "\n",
    "// Tokenize the entire text\n",
    "std::vector<int64_t> train_tokens = tokenizer_decoder.encode(data);\n",
    "\n",
    "// We'll train on short unrolled chunks\n",
    "const int64_t chunk_length = 64;\n",
    "const int64_t num_chunks = 10; // only do 3 steps as a demonstration\n",
    "\n",
    "// Optimizer (very simple, AdamW + a small learning rate)\n",
    "// Get parameters from the underlying model\n",
    "torch::optim::AdamW optimizer(\n",
    "    bert_model.is_distilbert() \n",
    "        ? bert_model.distilbert_model()->parameters()\n",
    "        : bert_model.bert_model()->parameters(),\n",
    "    torch::optim::AdamWOptions(5e-5)\n",
    ");\n",
    "\n",
    "// Set model to training mode\n",
    "if (bert_model.is_distilbert()) {\n",
    "    bert_model.distilbert_model()->train();\n",
    "} else {\n",
    "    bert_model.bert_model()->train();\n",
    "}\n",
    "for (int64_t c = 0; c < num_chunks; ++c) {\n",
    "    // Get chunk, unroll by chunk_length tokens\n",
    "    int64_t start = c * chunk_length;\n",
    "    if (start + chunk_length + 1 >= (int64_t)train_tokens.size()) break;\n",
    "    std::vector<int64_t> input_chunk(\n",
    "        train_tokens.begin() + start,\n",
    "        train_tokens.begin() + start + chunk_length\n",
    "    );\n",
    "    std::vector<int64_t> target_chunk(\n",
    "        train_tokens.begin() + start + 1,\n",
    "        train_tokens.begin() + start + chunk_length + 1\n",
    "    );\n",
    "\n",
    "    // Prepare tensors\n",
    "    torch::Tensor x = torch::from_blob(input_chunk.data(), {1, chunk_length}, torch::kInt64).clone();\n",
    "    torch::Tensor y = torch::from_blob(target_chunk.data(), {1, chunk_length}, torch::kInt64).clone();\n",
    "    torch::Tensor attention_mask = torch::ones({1, chunk_length}, torch::kInt64);\n",
    "\n",
    "    // Forward\n",
    "    torch::Tensor logits;\n",
    "    if (bert_model.is_distilbert()) {\n",
    "        logits = bert_model.distilbert_model()->forward(x, attention_mask); // [1, chunk_length, vocab_size]\n",
    "    } else {\n",
    "        logits = bert_model.bert_model()->forward(x, attention_mask);\n",
    "    }\n",
    "\n",
    "    // Compute loss (Cross Entropy over every position in the sequence)\n",
    "    logits = logits.view({-1, logits.size(-1)});\n",
    "    y = y.view({-1});\n",
    "    torch::Tensor loss = torch::nn::functional::cross_entropy(logits, y);\n",
    "\n",
    "    // Backward\n",
    "    optimizer.zero_grad();\n",
    "    loss.backward();\n",
    "    optimizer.step();\n",
    "\n",
    "    std::cout << \"Fine-tuning step \" << (c+1) << \" / \" << num_chunks << \", loss: \" << loss.item<float>() << std::endl;\n",
    "}\n",
    "\n",
    "// Set model back to evaluation mode\n",
    "if (bert_model.is_distilbert()) {\n",
    "    bert_model.distilbert_model()->eval();\n",
    "} else {\n",
    "    bert_model.bert_model()->eval();\n",
    "}\n",
    "std::cout << \"Done fine-tuning (demo: 3 steps on Shakespeare)!\" << std::endl;\n",
    "\n",
    "// Save the model after training\n",
    "try {\n",
    "    // You may choose your output filename. Here: \"fine_tuned_model.pt\"\n",
    "    torch::serialize::OutputArchive archive;\n",
    "    if (bert_model.is_distilbert()) {\n",
    "        bert_model.distilbert_model()->save(archive);\n",
    "    } else {\n",
    "        bert_model.bert_model()->save(archive);\n",
    "    }\n",
    "    archive.save_to(\"fine_tuned_model.pt\");\n",
    "    std::cout << \"Model saved to fine_tuned_model.pt\" << std::endl;\n",
    "} catch (const c10::Error& e) {\n",
    "    std::cerr << \"Error saving the model: \" << e.msg() << std::endl;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86fb4fd",
   "metadata": {},
   "source": [
    "## 7) Inference After Fine-tuning\n",
    "\n",
    "Now let's test the model again with the same prompt. Compare the output to see how fine-tuning has changed the model's behavior!\n",
    "\n",
    "**Compare with Section 4** to see the difference made by fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69c1d7",
   "metadata": {},
   "source": [
    "## 8) Save Fine-tuned Model\n",
    "\n",
    "Save the fine-tuned model checkpoint to disk so you can load it later or upload it to HuggingFace.\n",
    "\n",
    "The model is saved in PyTorch format (`.pt` file).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76d6f6a6",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The red fox\n",
      "Encoded input_ids: 101 1996 2417 4419 102 \n",
      "----\n",
      "Generated text:\n",
      "the red fox the [UNK] the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n"
     ]
    }
   ],
   "source": [
    "// Encode the prompt \"The red fox\"\n",
    "std::string prompt = \"The red fox\";\n",
    "std::vector<int64_t> input_ids = tokenizer_decoder.encode(prompt);\n",
    "// Display input tokens\n",
    "std::cout << \"Prompt: \" << prompt << std::endl;\n",
    "std::cout << \"Encoded input_ids: \";\n",
    "for (auto id : input_ids) std::cout << id << ' ';\n",
    "std::cout << std::endl;\n",
    "\n",
    "// Generate 100 new tokens (rudimentary greedy, no sampling)\n",
    "size_t max_tokens = 100;\n",
    "const int64_t SEP_TOKEN = 102;  // [SEP] token ID\n",
    "\n",
    "for (size_t t = 0; t < max_tokens; ++t) {\n",
    "    // Prepare input tensor from current input_ids [B=1, T=seq_len]\n",
    "    int64_t seq_len = static_cast<int64_t>(input_ids.size());\n",
    "    torch::Tensor input_tensor = torch::from_blob(\n",
    "        input_ids.data(), \n",
    "        {1, seq_len}, \n",
    "        torch::TensorOptions().dtype(torch::kInt64)\n",
    "    ).clone();\n",
    "    \n",
    "    // Create attention mask (1 for all tokens)\n",
    "    torch::Tensor attention_mask = torch::ones(\n",
    "        {1, seq_len}, \n",
    "        torch::TensorOptions().dtype(torch::kInt64)\n",
    "    );\n",
    "\n",
    "    // Forward pass to get logits\n",
    "    torch::NoGradGuard no_grad;\n",
    "    torch::Tensor logits;\n",
    "    if (bert_model.is_distilbert()) {\n",
    "        logits = bert_model.distilbert_model()->forward(input_tensor, attention_mask);\n",
    "    } else {\n",
    "        logits = bert_model.bert_model()->forward(input_tensor, attention_mask);\n",
    "    }\n",
    "    // logits shape: [1, seq_len, vocab_size]\n",
    "\n",
    "    // Get predicted token: argmax on the last position\n",
    "    auto last_logits = logits[0][seq_len - 1];  // [vocab_size]\n",
    "    int64_t next_token = torch::argmax(last_logits, -1).item<int64_t>();\n",
    "\n",
    "    // Append the predicted token\n",
    "    input_ids.push_back(next_token);\n",
    "\n",
    "    // Stop if SEP token (end of sequence)\n",
    "    if (next_token == SEP_TOKEN) break;\n",
    "}\n",
    "\n",
    "// Decode output\n",
    "std::string output_text = tokenizer_decoder.decode(input_ids);\n",
    "\n",
    "// Show the generated completion\n",
    "std::cout << \"----\\nGenerated text:\\n\" << output_text << std::endl;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7485969",
   "metadata": {},
   "source": [
    "## 9) Upload Fine-tuned Model to HuggingFace\n",
    "\n",
    "If you don’t want `huggingface-cli`, the simplest upload path is **git over HTTPS** (optionally with LFS for big files).\n",
    "\n",
    "**Upload process:**\n",
    "- Uses `git` over HTTPS (no Python required)\n",
    "- Automatically handles large files with `git-lfs`\n",
    "- Clones/pulls the repo, adds files, commits, and pushes\n",
    "\n",
    "**Auth:** Uses token from `secrets.txt` or `HF_TOKEN` environment variable.\n",
    "\n",
    "**Note:** Make sure you've completed Section 8 (Save Fine-tuned Model) first, so `fine_tuned_model.pt` exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d37ba",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found fine-tuned model: fine_tuned_model.pt\n",
      "File size: 359428976 bytes\n",
      "Loaded token from secrets.txt (length=37)\n",
      "\n",
      "=== Uploading Fine-tuned Model ===\n",
      "Repository: Warawreh/MCPPFA-demo-model\n",
      "File: fine_tuned_model.pt\n",
      "\n",
      "Starting upload (this may take a while for large models)...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#include <cstdlib>\n",
    "#include <cstdio>\n",
    "#include <iostream>\n",
    "#include <string>\n",
    "#include <filesystem>\n",
    "#include \"include/mcppfa/huggingface.hpp\"\n",
    "\n",
    "// Upload the fine-tuned model to HuggingFace Hub\n",
    "// Make sure you've run Section 8 (Save Fine-tuned Model) first!\n",
    "\n",
    "const std::string fine_tuned_model_path = \"fine_tuned_model.pt\";\n",
    "\n",
    "// Check if the fine-tuned model exists\n",
    "if (!std::filesystem::exists(fine_tuned_model_path)) {\n",
    "    throw std::runtime_error(\"Fine-tuned model not found: \" + fine_tuned_model_path + \n",
    "                              \"\\nPlease run Section 8 (Save Fine-tuned Model) first.\");\n",
    "}\n",
    "\n",
    "std::cout << \"Found fine-tuned model: \" << fine_tuned_model_path << std::endl;\n",
    "std::cout << \"File size: \" << std::filesystem::file_size(fine_tuned_model_path) << \" bytes\" << std::endl;\n",
    "\n",
    "// Upload to a model repo you own (change this to your repo)\n",
    "const std::string repo_id = \"Warawreh/MCPPFA-demo-model\";  // Update this to your HuggingFace username/repo\n",
    "\n",
    "// Prefer secrets.txt; fall back to env var HF_TOKEN\n",
    "std::string token;\n",
    "try {\n",
    "    token = mcppfa::hf::read_token_file(\"secrets.txt\");\n",
    "    std::cout << \"Loaded token from secrets.txt (length=\" << token.size() << \")\\n\";\n",
    "} catch (const std::exception& e) {\n",
    "    std::cerr << \"Could not read secrets.txt: \" << e.what() << \"\\n\";\n",
    "    if (const char* tok = std::getenv(\"HF_TOKEN\")) {\n",
    "        token = tok;\n",
    "        std::cout << \"Using token from HF_TOKEN environment variable\\n\";\n",
    "    }\n",
    "}\n",
    "\n",
    "try {\n",
    "    if (token.empty()) {\n",
    "        std::cerr << \"ERROR: Provide a token via secrets.txt or HF_TOKEN to upload.\" << std::endl;\n",
    "        std::cerr << \"Create secrets.txt with your HuggingFace token on the first line.\" << std::endl;\n",
    "    } else {\n",
    "        mcppfa::hf::GitUploadOptions opt;\n",
    "        opt.use_lfs = true;  // Use git-lfs for large model files\n",
    "        opt.stream_progress = true;\n",
    "        opt.max_stream_bytes = 300000; // keep notebook output reasonable\n",
    "\n",
    "        std::cout << \"\\n=== Uploading Fine-tuned Model ===\" << std::endl;\n",
    "        std::cout << \"Repository: \" << repo_id << std::endl;\n",
    "        std::cout << \"File: \" << fine_tuned_model_path << std::endl;\n",
    "        std::cout << \"\\nStarting upload (this may take a while for large models)...\\n\" << std::endl;\n",
    "\n",
    "        // Upload the fine-tuned model checkpoint\n",
    "        mcppfa::hf::HubUploader hub(repo_id, mcppfa::hf::RepoType::model, token, opt);\n",
    "        hub.upload(fine_tuned_model_path, \"fine_tuned_model.pt\");\n",
    "        \n",
    "        auto log = hub.push(\"upload fine-tuned BERT model from C++\");\n",
    "        \n",
    "        std::cout << \"\\n=== Upload Complete ===\" << std::endl;\n",
    "        std::cout << \"Exit status: \" << log.exit_status << std::endl;\n",
    "        std::cout << \"System return code: \" << log.system_rc << std::endl;\n",
    "        std::cout << \"Total time: \" << log.seconds_total << \" seconds\" << std::endl;\n",
    "        std::cout << \"\\nUpload report:\\n\" << log.report << std::endl;\n",
    "        \n",
    "        if (log.exit_status == 0) {\n",
    "            std::cout << \"\\n✓ Successfully uploaded fine-tuned model to HuggingFace Hub!\" << std::endl;\n",
    "            std::cout << \"View it at: https://huggingface.co/\" << repo_id << std::endl;\n",
    "        } else {\n",
    "            std::cerr << \"\\n✗ Upload may have failed. Check the report above for details.\" << std::endl;\n",
    "        }\n",
    "    }\n",
    "} catch (const std::exception& e) {\n",
    "    std::cerr << \"Upload failed: \" << e.what() << std::endl;\n",
    "    std::cerr << \"\\nTroubleshooting:\" << std::endl;\n",
    "    std::cerr << \"1. Make sure git-lfs is installed: sudo apt-get install -y git-lfs && git lfs install\" << std::endl;\n",
    "    std::cerr << \"2. Verify your HuggingFace token is valid\" << std::endl;\n",
    "    std::cerr << \"3. Check that the repository \" << repo_id << \" exists and you have write access\" << std::endl;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6599a9",
   "metadata": {},
   "source": [
    "## Notes / limitations\n",
    "\n",
    "- If `huggingface-cli` returns `32512`, that’s usually `127 << 8` → **command not found** (the binary isn’t on PATH).\n",
    "- The **no-Python path** in this notebook uses `curl` for downloads and `git` for uploads, both invoked from C++.\n",
    "- Upload via git can require `git-lfs` for large model files. Install it if needed: `sudo apt-get install -y git-lfs` then run `git lfs install`.\n",
    "- Embedding `HF_TOKEN` into an HTTPS git URL is convenient but can leak tokens (process list/history). Prefer credential helpers when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd83b0a",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++17",
   "name": "xcpp17"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
