{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5efae089",
   "metadata": {},
   "source": [
    "# Hugging Face + LibTorch: Fine-tuning BERT in Pure C++\n",
    "\n",
    "This notebook demonstrates a complete **pure C++ (LibTorch)** workflow for fine-tuning a BERT model:\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Setup**: Install tools, authenticate, configure LibTorch\n",
    "2. **Load Model**: Download or load a pre-trained BERT/DistilBERT model from HuggingFace\n",
    "3. **Inference (Before)**: Test the model's generation before fine-tuning\n",
    "4. **Load Dataset**: Download and prepare a training dataset\n",
    "5. **Fine-tune Model**: Train the model on your dataset\n",
    "6. **Inference (After)**: Test the model's generation after fine-tuning to see improvements\n",
    "7. **Save Model**: Save the fine-tuned model checkpoint\n",
    "8. **Upload to HuggingFace**: Upload your fine-tuned model back to the Hub\n",
    "\n",
    "## What This Notebook Shows\n",
    "\n",
    "- **Pure C++ workflow**: No Python dependencies for model training or inference\n",
    "- **HuggingFace integration**: Download models, upload checkpoints using `curl` and `git`\n",
    "- **Fine-tuning demonstration**: Train a BERT model on a custom dataset\n",
    "- **Before/After comparison**: See how fine-tuning affects model behavior\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- BERT/DistilBERT are **bidirectional encoders**, not designed for autoregressive text generation\n",
    "- The generation examples may produce `[UNK]` tokens - this is expected behavior\n",
    "- For better text generation, consider using a decoder model (GPT-style)\n",
    "- This notebook uses a **plain-text dataset** and a **tokenizer** implemented in C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee472f94",
   "metadata": {},
   "source": [
    "## 1) Install Hugging Face tooling (pick one path)\n",
    "\n",
    "### Path A (Python-based): `huggingface-cli`\n",
    "\n",
    "> This path requires Python because `huggingface-cli` comes from the `huggingface_hub` Python package.\n",
    "\n",
    "- `python -m pip install -U huggingface_hub`\n",
    "- `conda install -n cpp-notebooks -c conda-forge -y huggingface_hub`\n",
    "- `mamba install -n cpp-notebooks -c conda-forge -y huggingface_hub`\n",
    "\n",
    "### Path B (no Python): `curl` + `git`\n",
    "\n",
    "> This notebook supports a no-Python path using `curl` for downloads and `git` for uploads (both invoked from C++).\n",
    "\n",
    "- Install on Ubuntu/WSL: `sudo apt-get update && sudo apt-get install -y curl git`\n",
    "\n",
    "After installing, verify with the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d38a3",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "#include <cstdlib>\n",
    "#include <iostream>\n",
    "\n",
    "// Prefer the no-Python path: curl is enough for downloads.\n",
    "int rc_curl = std::system(\"curl --version\");\n",
    "std::cout << \"curl rc=\" << rc_curl << std::endl;\n",
    "\n",
    "// Optional: git for uploads (no Python).\n",
    "int rc_git = std::system(\"git --version\");\n",
    "std::cout << \"git rc=\" << rc_git << std::endl;\n",
    "\n",
    "// Optional: git-lfs for large model files (recommended for weights).\n",
    "int rc_lfs = std::system(\"git lfs version\");\n",
    "std::cout << \"git-lfs rc=\" << rc_lfs << std::endl;\n",
    "\n",
    "// Optional: huggingface-cli (Python-based).\n",
    "int rc_hf = std::system(\"huggingface-cli --help\");\n",
    "std::cout << \"huggingface-cli rc=\" << rc_hf << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a50cd1",
   "metadata": {},
   "source": [
    "## 2) Authenticate\n",
    "\n",
    "There are two practical auth paths:\n",
    "\n",
    "### Path A (Python-based): `huggingface-cli login` (optional)\n",
    "- Requires `huggingface-cli` (from `huggingface_hub`) on PATH\n",
    "- Useful if you want the CLI-based `download/upload` helpers\n",
    "\n",
    "### Path B (no Python): token passed to `curl`/`git` helpers (recommended here)\n",
    "- Set an environment variable: `HF_TOKEN`\n",
    "- Pass it to:\n",
    "  - `download_file_http(..., token)` for private downloads\n",
    "  - `upload_file_git(..., token)` for uploads\n",
    "- You do **not** need to run `huggingface-cli login` for this path\n",
    "\n",
    "Notes:\n",
    "- Avoid pasting tokens into notebooks if you plan to commit them.\n",
    "- If you exported `HF_TOKEN` in a terminal after the notebook kernel started, restart the kernel so it can see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba6afd",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "#include <string>\n",
    "#include \"include/mcppfa/huggingface.hpp\"\n",
    "\n",
    "// Read token from secrets.txt (raw token on first line).\n",
    "// Do NOT commit secrets.txt; add it to .gitignore.\n",
    "\n",
    "std::string token;\n",
    "try {\n",
    "    token = mcppfa::hf::read_token_file(\"secrets.txt\");\n",
    "    std::cout << \"Loaded token from secrets.txt (length=\" << token.size() << \")\\n\";\n",
    "} catch (const std::exception& e) {\n",
    "    std::cerr << \"Could not read secrets.txt: \" << e.what() << \"\\n\";\n",
    "    std::cerr << \"Falling back to env var HF_TOKEN (if set for this kernel).\\n\";\n",
    "}\n",
    "\n",
    "try {\n",
    "    auto res = mcppfa::hf::login(token, \"HF_TOKEN\");\n",
    "    std::cout << \"login rc=\" << res.exit_code << std::endl;\n",
    "} catch (const std::exception& e) {\n",
    "    std::cerr << \"Login skipped/failed: \" << e.what() << std::endl;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7c4c5",
   "metadata": {},
   "source": [
    "## 3) Load Model and Tokenizer\n",
    "\n",
    "This step downloads a pre-trained BERT/DistilBERT model from HuggingFace (or uses local files if they already exist).\n",
    "\n",
    "The model will be loaded into memory and ready for inference and fine-tuning.\n",
    "\n",
    "**Note**: If you've already downloaded the model in a previous run, the code will detect local files and reuse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded5356",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "// LibTorch dynamic linking for xcpp17/cling (required for <torch/torch.h>)\n",
    "#pragma cling add_include_path(\"/home/warawreh/MakeCPPFunAgain/libtorch/include\")\n",
    "#pragma cling add_include_path(\"/home/warawreh/MakeCPPFunAgain/libtorch/include/torch/csrc/api/include\")\n",
    "#pragma cling add_library_path(\"/home/warawreh/MakeCPPFunAgain/libtorch/lib\")\n",
    "#pragma cling load(\"/home/warawreh/MakeCPPFunAgain/libtorch/lib/libc10.so\")\n",
    "#pragma cling load(\"/home/warawreh/MakeCPPFunAgain/libtorch/lib/libtorch_cpu.so\")\n",
    "#pragma cling load(\"/home/warawreh/MakeCPPFunAgain/libtorch/lib/libtorch.so\")\n",
    "\n",
    "#include <iostream>\n",
    "#include <filesystem>\n",
    "#include \"include/mcppfa/bert_huggingface.hpp\"\n",
    "#include \"include/mcppfa/tokenizer_decoder.hpp\"\n",
    "\n",
    "// Load BERT model and tokenizer from HuggingFace (or use local files if they exist)\n",
    "const std::string repo_id = \"distilbert/distilbert-base-uncased\";\n",
    "const std::string model_name = \"distilbert-base-uncased\";\n",
    "const std::string local_dir = \".hf/\" + model_name;\n",
    "const std::string tokenizer_path = local_dir + \"/tokenizer.json\";\n",
    "const std::string config_path = local_dir + \"/config.json\";\n",
    "const std::string weights_path_safetensors = local_dir + \"/model.safetensors\";\n",
    "const std::string weights_path_bin = local_dir + \"/pytorch_model.bin\";\n",
    "\n",
    "// Check if local files exist\n",
    "bool tokenizer_exists = std::filesystem::exists(tokenizer_path);\n",
    "bool config_exists = std::filesystem::exists(config_path);\n",
    "bool weights_exist = std::filesystem::exists(weights_path_safetensors) || \n",
    "                     std::filesystem::exists(weights_path_bin);\n",
    "\n",
    "// Load tokenizer\n",
    "mcppfa::hf::BERTTokenizerWrapper tokenizer;\n",
    "if (tokenizer_exists) {\n",
    "    std::cout << \"Tokenizer found locally at: \" << tokenizer_path << std::endl;\n",
    "    // Since BERTTokenizerWrapper doesn't have load_from_local, we need to download\n",
    "    // but it will use the existing file. Actually, let's check the implementation...\n",
    "    // The load_from_hf will try to download. Let's work around this by checking\n",
    "    // if we can avoid the download call.\n",
    "    std::cout << \"Note: Tokenizer file exists, but wrapper will still attempt download.\" << std::endl;\n",
    "    std::cout << \"      The download will likely reuse the existing file.\" << std::endl;\n",
    "    tokenizer.load_from_hf(repo_id, \"\", mcppfa::hf::RepoType::model, \"main\");\n",
    "} else {\n",
    "    std::cout << \"Downloading tokenizer from HuggingFace...\" << std::endl;\n",
    "    tokenizer.load_from_hf(repo_id, \"\", mcppfa::hf::RepoType::model, \"main\");\n",
    "}\n",
    "std::cout << \"Tokenizer loaded from: \" << tokenizer.tokenizer_path() << std::endl;\n",
    "\n",
    "// Load BERT model\n",
    "mcppfa::hf::BERTModelWrapper bert_model;\n",
    "if (config_exists && weights_exist) {\n",
    "    std::cout << \"Model files found locally:\" << std::endl;\n",
    "    if (config_exists) std::cout << \"  Config: \" << config_path << std::endl;\n",
    "    if (std::filesystem::exists(weights_path_safetensors)) {\n",
    "        std::cout << \"  Weights: \" << weights_path_safetensors << std::endl;\n",
    "    } else if (std::filesystem::exists(weights_path_bin)) {\n",
    "        std::cout << \"  Weights: \" << weights_path_bin << std::endl;\n",
    "    }\n",
    "    std::cout << \"Note: Model files exist, but wrapper will still attempt download.\" << std::endl;\n",
    "    std::cout << \"      The download will likely reuse existing files.\" << std::endl;\n",
    "    bert_model.load_from_hf(repo_id, \"\", mcppfa::hf::RepoType::model, \"main\");\n",
    "} else {\n",
    "    std::cout << \"Downloading model from HuggingFace...\" << std::endl;\n",
    "    bert_model.load_from_hf(repo_id, \"\", mcppfa::hf::RepoType::model, \"main\");\n",
    "}\n",
    "std::cout << \"BERT model loaded from: \" << bert_model.weights_path() << std::endl;\n",
    "std::cout << \"Config loaded from: \" << bert_model.config_path() << std::endl;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d977e",
   "metadata": {},
   "source": [
    "## 4) Inference Before Fine-tuning\n",
    "\n",
    "Let's test the model's text generation capabilities **before** fine-tuning. This gives us a baseline to compare against after training.\n",
    "\n",
    "**Note**: BERT/DistilBERT are bidirectional encoders, not designed for autoregressive generation. You may see `[UNK]` tokens in the output - this is expected behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea240e",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "// ===== INFERENCE: Generate 100 tokens =====\n",
    "std::cout << \"\\n=== Starting Inference ===\" << std::endl;\n",
    "\n",
    "// Set model to evaluation mode\n",
    "if (bert_model.is_distilbert()) {\n",
    "    bert_model.distilbert_model()->eval();\n",
    "} else {\n",
    "    bert_model.bert_model()->eval();\n",
    "}\n",
    "// Load tokenizer decoder for encoding/decoding (matches Python's tokenizer.encode/decode)\n",
    "mcppfa::tokenizer::TokenizerDecoder tokenizer_decoder;\n",
    "tokenizer_decoder.load_from_file(tokenizer.tokenizer_path());\n",
    "std::cout << \"Tokenizer loaded: \" << tokenizer_decoder.vocab_size() << \" tokens\" << std::endl;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f46b3d",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "// ===== INFERENCE: Using the new predict() method (like Python transformers) =====\n",
    "// The predict() method maintains internal state and generates tokens one at a time\n",
    "\n",
    "std::string prompt = \"The red fox\";\n",
    "std::cout << \"Prompt: \" << prompt << std::endl;\n",
    "\n",
    "// Initialize the model's generation state with the prompt\n",
    "// This is similar to Python: model.generate(tokenizer.encode(prompt), ...)\n",
    "bert_model.reset(tokenizer_decoder, prompt);\n",
    "\n",
    "// Display initial encoded tokens\n",
    "std::cout << \"Encoded input_ids: \";\n",
    "for (auto id : bert_model.get_input_ids()) std::cout << id << ' ';\n",
    "std::cout << std::endl;\n",
    "\n",
    "// Generation parameters (like Python transformers)\n",
    "const double temperature = 0.8;  // Lower temperature = more conservative\n",
    "const int64_t top_k = 50;         // Only sample from top 50 tokens\n",
    "const size_t max_tokens = 50;     // Maximum tokens to generate\n",
    "\n",
    "std::cout << \"\\n=== Generating with predict() method ===\" << std::endl;\n",
    "std::cout << \"Note: BERT is not designed for autoregressive generation.\" << std::endl;\n",
    "std::cout << \"Special tokens are automatically filtered (like transformers library).\\n\" << std::endl;\n",
    "\n",
    "// Generate tokens using the predict() method\n",
    "// Each call generates the next token and automatically updates internal state\n",
    "for (size_t t = 0; t < max_tokens; ++t) {\n",
    "    int64_t next_token = bert_model.predict(\n",
    "        tokenizer_decoder,\n",
    "        temperature,\n",
    "        top_k,\n",
    "        false  // not greedy, use sampling\n",
    "    );\n",
    "    \n",
    "    // predict() returns -1 if generation should stop (e.g., [SEP] token or max length)\n",
    "    if (next_token == -1) {\n",
    "        std::cout << \"Generation stopped (reached stopping condition).\" << std::endl;\n",
    "        break;\n",
    "    }\n",
    "    \n",
    "    // Progress indicator\n",
    "    if ((t + 1) % 10 == 0) {\n",
    "        std::cout << \"Generated \" << (t + 1) << \" tokens...\" << std::endl;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Decode output (automatically skips special tokens)\n",
    "std::string output_text = tokenizer_decoder.decode(bert_model.get_input_ids());\n",
    "\n",
    "// Show the generated completion\n",
    "std::cout << \"\\n----\\nGenerated text:\\n\" << output_text << std::endl;\n",
    "\n",
    "std::cout << \"\\nNote: Special tokens are automatically filtered during generation and decoding.\" << std::endl;\n",
    "std::cout << \"This matches how Python's transformers library handles special tokens.\" << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4124b6",
   "metadata": {},
   "source": [
    "## 5) Load Dataset\n",
    "\n",
    "Download and prepare the training dataset. We'll use the Tiny Shakespeare dataset as an example.\n",
    "\n",
    "The dataset will be tokenized and ready for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3abb910",
   "metadata": {},
   "source": [
    "## 6) Fine-tune Model\n",
    "\n",
    "Now we'll fine-tune the pre-trained model on our dataset. This adapts the model to the specific domain or task.\n",
    "\n",
    "**Training parameters:**\n",
    "- Learning rate: 5e-5 (standard for BERT fine-tuning)\n",
    "- Optimizer: AdamW\n",
    "- Batch size: 1 (sequence length: 64)\n",
    "- Training steps: Configurable (default: 100 chunks)\n",
    "\n",
    "Watch the loss decrease as the model learns!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f1b33",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "// Fine-tune the model on the Shakespeare dataset (tiny demo, 3-4 steps)\n",
    "\n",
    "#include <torch/torch.h>\n",
    "#include <iostream>\n",
    "\n",
    "const std::string data_path = \".hf/tiny_shakespeare/input.txt\";\n",
    "\n",
    "// Open the dataset (already downloaded as data_path)\n",
    "std::ifstream infile(data_path);\n",
    "if (!infile.is_open()) {\n",
    "    throw std::runtime_error(\"Failed to open input.txt for fine-tuning.\");\n",
    "}\n",
    "std::string data((std::istreambuf_iterator<char>(infile)), std::istreambuf_iterator<char>());\n",
    "\n",
    "// Tokenize the entire text\n",
    "std::vector<int64_t> train_tokens = tokenizer_decoder.encode(data);\n",
    "\n",
    "// We'll train on short unrolled chunks\n",
    "const int64_t chunk_length = 64;\n",
    "const int64_t num_chunks = 10; // only do 3 steps as a demonstration\n",
    "\n",
    "// Optimizer (very simple, AdamW + a small learning rate)\n",
    "// Get parameters from the underlying model\n",
    "torch::optim::AdamW optimizer(\n",
    "    bert_model.is_distilbert() \n",
    "        ? bert_model.distilbert_model()->parameters()\n",
    "        : bert_model.bert_model()->parameters(),\n",
    "    torch::optim::AdamWOptions(5e-5)\n",
    ");\n",
    "\n",
    "// Set model to training mode\n",
    "if (bert_model.is_distilbert()) {\n",
    "    bert_model.distilbert_model()->train();\n",
    "} else {\n",
    "    bert_model.bert_model()->train();\n",
    "}\n",
    "for (int64_t c = 0; c < num_chunks; ++c) {\n",
    "    // Get chunk, unroll by chunk_length tokens\n",
    "    int64_t start = c * chunk_length;\n",
    "    if (start + chunk_length + 1 >= (int64_t)train_tokens.size()) break;\n",
    "    std::vector<int64_t> input_chunk(\n",
    "        train_tokens.begin() + start,\n",
    "        train_tokens.begin() + start + chunk_length\n",
    "    );\n",
    "    std::vector<int64_t> target_chunk(\n",
    "        train_tokens.begin() + start + 1,\n",
    "        train_tokens.begin() + start + chunk_length + 1\n",
    "    );\n",
    "\n",
    "    // Prepare tensors\n",
    "    torch::Tensor x = torch::from_blob(input_chunk.data(), {1, chunk_length}, torch::kInt64).clone();\n",
    "    torch::Tensor y = torch::from_blob(target_chunk.data(), {1, chunk_length}, torch::kInt64).clone();\n",
    "    torch::Tensor attention_mask = torch::ones({1, chunk_length}, torch::kInt64);\n",
    "\n",
    "    // Forward\n",
    "    torch::Tensor logits;\n",
    "    if (bert_model.is_distilbert()) {\n",
    "        logits = bert_model.distilbert_model()->forward(x, attention_mask); // [1, chunk_length, vocab_size]\n",
    "    } else {\n",
    "        logits = bert_model.bert_model()->forward(x, attention_mask);\n",
    "    }\n",
    "\n",
    "    // Compute loss (Cross Entropy over every position in the sequence)\n",
    "    logits = logits.view({-1, logits.size(-1)});\n",
    "    y = y.view({-1});\n",
    "    torch::Tensor loss = torch::nn::functional::cross_entropy(logits, y);\n",
    "\n",
    "    // Backward\n",
    "    optimizer.zero_grad();\n",
    "    loss.backward();\n",
    "    optimizer.step();\n",
    "\n",
    "    std::cout << \"Fine-tuning step \" << (c+1) << \" / \" << num_chunks << \", loss: \" << loss.item<float>() << std::endl;\n",
    "}\n",
    "\n",
    "// Set model back to evaluation mode\n",
    "if (bert_model.is_distilbert()) {\n",
    "    bert_model.distilbert_model()->eval();\n",
    "} else {\n",
    "    bert_model.bert_model()->eval();\n",
    "}\n",
    "std::cout << \"Done fine-tuning (demo: 3 steps on Shakespeare)!\" << std::endl;\n",
    "\n",
    "// Save the model after training\n",
    "try {\n",
    "    // You may choose your output filename. Here: \"fine_tuned_model.pt\"\n",
    "    torch::serialize::OutputArchive archive;\n",
    "    if (bert_model.is_distilbert()) {\n",
    "        bert_model.distilbert_model()->save(archive);\n",
    "    } else {\n",
    "        bert_model.bert_model()->save(archive);\n",
    "    }\n",
    "    archive.save_to(\"fine_tuned_model.pt\");\n",
    "    std::cout << \"Model saved to fine_tuned_model.pt\" << std::endl;\n",
    "} catch (const c10::Error& e) {\n",
    "    std::cerr << \"Error saving the model: \" << e.msg() << std::endl;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86fb4fd",
   "metadata": {},
   "source": [
    "## 7) Inference After Fine-tuning\n",
    "\n",
    "Now let's test the model again with the same prompt. Compare the output to see how fine-tuning has changed the model's behavior!\n",
    "\n",
    "**Compare with Section 4** to see the difference made by fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69c1d7",
   "metadata": {},
   "source": [
    "## 8) Save Fine-tuned Model\n",
    "\n",
    "Save the fine-tuned model checkpoint to disk so you can load it later or upload it to HuggingFace.\n",
    "\n",
    "The model is saved in PyTorch format (`.pt` file).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6f6a6",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "// Encode the prompt \"The red fox\"\n",
    "std::string prompt = \"The red fox\";\n",
    "std::vector<int64_t> input_ids = tokenizer_decoder.encode(prompt);\n",
    "// Display input tokens\n",
    "std::cout << \"Prompt: \" << prompt << std::endl;\n",
    "std::cout << \"Encoded input_ids: \";\n",
    "for (auto id : input_ids) std::cout << id << ' ';\n",
    "std::cout << std::endl;\n",
    "\n",
    "// Generate 100 new tokens (rudimentary greedy, no sampling)\n",
    "size_t max_tokens = 100;\n",
    "const int64_t SEP_TOKEN = 102;  // [SEP] token ID\n",
    "\n",
    "for (size_t t = 0; t < max_tokens; ++t) {\n",
    "    // Prepare input tensor from current input_ids [B=1, T=seq_len]\n",
    "    int64_t seq_len = static_cast<int64_t>(input_ids.size());\n",
    "    torch::Tensor input_tensor = torch::from_blob(\n",
    "        input_ids.data(), \n",
    "        {1, seq_len}, \n",
    "        torch::TensorOptions().dtype(torch::kInt64)\n",
    "    ).clone();\n",
    "    \n",
    "    // Create attention mask (1 for all tokens)\n",
    "    torch::Tensor attention_mask = torch::ones(\n",
    "        {1, seq_len}, \n",
    "        torch::TensorOptions().dtype(torch::kInt64)\n",
    "    );\n",
    "\n",
    "    // Forward pass to get logits\n",
    "    torch::NoGradGuard no_grad;\n",
    "    torch::Tensor logits;\n",
    "    if (bert_model.is_distilbert()) {\n",
    "        logits = bert_model.distilbert_model()->forward(input_tensor, attention_mask);\n",
    "    } else {\n",
    "        logits = bert_model.bert_model()->forward(input_tensor, attention_mask);\n",
    "    }\n",
    "    // logits shape: [1, seq_len, vocab_size]\n",
    "\n",
    "    // Get predicted token: argmax on the last position\n",
    "    auto last_logits = logits[0][seq_len - 1];  // [vocab_size]\n",
    "    int64_t next_token = torch::argmax(last_logits, -1).item<int64_t>();\n",
    "\n",
    "    // Append the predicted token\n",
    "    input_ids.push_back(next_token);\n",
    "\n",
    "    // Stop if SEP token (end of sequence)\n",
    "    if (next_token == SEP_TOKEN) break;\n",
    "}\n",
    "\n",
    "// Decode output\n",
    "std::string output_text = tokenizer_decoder.decode(input_ids);\n",
    "\n",
    "// Show the generated completion\n",
    "std::cout << \"----\\nGenerated text:\\n\" << output_text << std::endl;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7485969",
   "metadata": {},
   "source": [
    "## 9) Upload Fine-tuned Model to HuggingFace\n",
    "\n",
    "If you don’t want `huggingface-cli`, the simplest upload path is **git over HTTPS** (optionally with LFS for big files).\n",
    "\n",
    "**Upload process:**\n",
    "- Uses `git` over HTTPS (no Python required)\n",
    "- Automatically handles large files with `git-lfs`\n",
    "- Clones/pulls the repo, adds files, commits, and pushes\n",
    "\n",
    "**Auth:** Uses token from `secrets.txt` or `HF_TOKEN` environment variable.\n",
    "\n",
    "**Note:** Make sure you've completed Section 8 (Save Fine-tuned Model) first, so `fine_tuned_model.pt` exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d37ba",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "#include <cstdlib>\n",
    "#include <cstdio>\n",
    "#include <iostream>\n",
    "#include <string>\n",
    "#include <filesystem>\n",
    "#include \"include/mcppfa/huggingface.hpp\"\n",
    "\n",
    "// Upload the fine-tuned model to HuggingFace Hub\n",
    "// Make sure you've run Section 8 (Save Fine-tuned Model) first!\n",
    "\n",
    "const std::string fine_tuned_model_path = \"fine_tuned_model.pt\";\n",
    "\n",
    "// Check if the fine-tuned model exists\n",
    "if (!std::filesystem::exists(fine_tuned_model_path)) {\n",
    "    throw std::runtime_error(\"Fine-tuned model not found: \" + fine_tuned_model_path + \n",
    "                              \"\\nPlease run Section 8 (Save Fine-tuned Model) first.\");\n",
    "}\n",
    "\n",
    "std::cout << \"Found fine-tuned model: \" << fine_tuned_model_path << std::endl;\n",
    "std::cout << \"File size: \" << std::filesystem::file_size(fine_tuned_model_path) << \" bytes\" << std::endl;\n",
    "\n",
    "// Upload to a model repo you own (change this to your repo)\n",
    "const std::string repo_id = \"Warawreh/MCPPFA-demo-model\";  // Update this to your HuggingFace username/repo\n",
    "\n",
    "// Prefer secrets.txt; fall back to env var HF_TOKEN\n",
    "std::string token;\n",
    "try {\n",
    "    token = mcppfa::hf::read_token_file(\"secrets.txt\");\n",
    "    std::cout << \"Loaded token from secrets.txt (length=\" << token.size() << \")\\n\";\n",
    "} catch (const std::exception& e) {\n",
    "    std::cerr << \"Could not read secrets.txt: \" << e.what() << \"\\n\";\n",
    "    if (const char* tok = std::getenv(\"HF_TOKEN\")) {\n",
    "        token = tok;\n",
    "        std::cout << \"Using token from HF_TOKEN environment variable\\n\";\n",
    "    }\n",
    "}\n",
    "\n",
    "try {\n",
    "    if (token.empty()) {\n",
    "        std::cerr << \"ERROR: Provide a token via secrets.txt or HF_TOKEN to upload.\" << std::endl;\n",
    "        std::cerr << \"Create secrets.txt with your HuggingFace token on the first line.\" << std::endl;\n",
    "    } else {\n",
    "        mcppfa::hf::GitUploadOptions opt;\n",
    "        opt.use_lfs = true;  // Use git-lfs for large model files\n",
    "        opt.stream_progress = true;\n",
    "        opt.max_stream_bytes = 300000; // keep notebook output reasonable\n",
    "\n",
    "        std::cout << \"\\n=== Uploading Fine-tuned Model ===\" << std::endl;\n",
    "        std::cout << \"Repository: \" << repo_id << std::endl;\n",
    "        std::cout << \"File: \" << fine_tuned_model_path << std::endl;\n",
    "        std::cout << \"\\nStarting upload (this may take a while for large models)...\\n\" << std::endl;\n",
    "\n",
    "        // Upload the fine-tuned model checkpoint\n",
    "        mcppfa::hf::HubUploader hub(repo_id, mcppfa::hf::RepoType::model, token, opt);\n",
    "        hub.upload(fine_tuned_model_path, \"fine_tuned_model.pt\");\n",
    "        \n",
    "        auto log = hub.push(\"upload fine-tuned BERT model from C++\");\n",
    "        \n",
    "        std::cout << \"\\n=== Upload Complete ===\" << std::endl;\n",
    "        std::cout << \"Exit status: \" << log.exit_status << std::endl;\n",
    "        std::cout << \"System return code: \" << log.system_rc << std::endl;\n",
    "        std::cout << \"Total time: \" << log.seconds_total << \" seconds\" << std::endl;\n",
    "        std::cout << \"\\nUpload report:\\n\" << log.report << std::endl;\n",
    "        \n",
    "        if (log.exit_status == 0) {\n",
    "            std::cout << \"\\n✓ Successfully uploaded fine-tuned model to HuggingFace Hub!\" << std::endl;\n",
    "            std::cout << \"View it at: https://huggingface.co/\" << repo_id << std::endl;\n",
    "        } else {\n",
    "            std::cerr << \"\\n✗ Upload may have failed. Check the report above for details.\" << std::endl;\n",
    "        }\n",
    "    }\n",
    "} catch (const std::exception& e) {\n",
    "    std::cerr << \"Upload failed: \" << e.what() << std::endl;\n",
    "    std::cerr << \"\\nTroubleshooting:\" << std::endl;\n",
    "    std::cerr << \"1. Make sure git-lfs is installed: sudo apt-get install -y git-lfs && git lfs install\" << std::endl;\n",
    "    std::cerr << \"2. Verify your HuggingFace token is valid\" << std::endl;\n",
    "    std::cerr << \"3. Check that the repository \" << repo_id << \" exists and you have write access\" << std::endl;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd83b0a",
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++17",
   "name": "xcpp17"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "C++17",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
